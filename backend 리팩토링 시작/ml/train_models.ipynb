{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebc97172-6d15-4eac-a998-a6af16a12495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Tracking URI: file:./mlruns\n",
      "MLflow Experiment: cafe24-ops-ai\n",
      "======================================================================\n",
      "PART 1: 설정 완료\n",
      "  BACKEND_DIR: C:\\Users\\AKS\\Desktop\\카페24 프로젝트\\backend 리팩토링 시작\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PART 2: 데이터 생성 (카페24 이커머스)\n",
      "======================================================================\n",
      "\n",
      "[2.1] 쇼핑몰 데이터 생성\n",
      "  - 쇼핑몰: 300개 (active: 216, dormant: 48, churned: 36)\n",
      "\n",
      "[2.2] 카테고리 데이터 생성\n",
      "  - 카테고리: 8개\n",
      "\n",
      "[2.3] 서비스 데이터 생성\n",
      "  - 서비스: 1163건\n",
      "\n",
      "[2.4] 상품 데이터 생성\n",
      "  - 상품: 7285개\n",
      "\n",
      "[2.5] 셀러 데이터 생성\n",
      "  - 셀러: 300명\n",
      "\n",
      "[2.6] 운영 로그 데이터 생성\n",
      "  - 운영 로그: 24720건\n",
      "\n",
      "[2.7] 셀러 분석 데이터 생성\n",
      "  - 셀러 분석: 300명\n",
      "\n",
      "[2.8] 쇼핑몰 성과 데이터 생성\n",
      "  - 쇼핑몰 성과: 300개\n",
      "\n",
      "[2.9] 일별 플랫폼 지표 생성\n",
      "  - 일별 지표: 90일\n",
      "\n",
      "[2.10] CS 통계 데이터 생성\n",
      "  - CS 통계: 9개 카테고리\n",
      "\n",
      "[2.11] 이상거래 상세 데이터 생성\n",
      "  - 이상거래 상세: 15건\n",
      "\n",
      "[2.12] 코호트 리텐션 데이터 생성\n",
      "  - 코호트 리텐션: 6개월\n",
      "  - 전환 퍼널: 6개월\n",
      "\n",
      "[2.13] 셀러 일별 활동 데이터 생성\n",
      "  - 셀러 일별 활동: 27000건\n",
      "\n",
      "[2.14] 플랫폼 문서 데이터 생성\n",
      "  - 플랫폼 문서: 12건\n",
      "\n",
      "[2.15] 이커머스 용어 사전 생성\n",
      "  - 용어 사전: 15건\n",
      "\n",
      "[2.16] 셀러-상품 매핑 데이터 생성\n",
      "  - 셀러 상품: 7285건\n",
      "\n",
      "[2.17] 셀러 리소스 데이터 생성\n",
      "  - 셀러 리소스: 300건\n",
      "\n",
      "======================================================================\n",
      "PART 2 완료: 모든 데이터 생성 완료 (18개 CSV)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PART 3: 모델 학습 (10개)\n",
      "======================================================================\n",
      "\n",
      "[3.1] 셀러 이탈 예측 모델 (RandomForest + SHAP)\n",
      "  정확도: 0.9833, F1: 0.9333\n",
      "  SHAP 분석 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 21:52:12 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "Registered model '셀러이탈예측' already exists. Creating a new version of this model...\n",
      "Created version '3' of model '셀러이탈예측'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.2] 이상거래 탐지 모델 (Isolation Forest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 21:52:23 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  이상 셀러: 15명 (5.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model '이상거래탐지' already exists. Creating a new version of this model...\n",
      "Created version '3' of model '이상거래탐지'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.3] 문의 자동 분류 모델 (TF-IDF + RandomForest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 21:52:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  정확도: 1.0000, F1(매크로): 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model '문의자동분류' already exists. Creating a new version of this model...\n",
      "Created version '3' of model '문의자동분류'.\n",
      "2026/02/09 21:52:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.4] 셀러 세그먼트 모델 (K-Means)\n",
      "  실루엣 점수: 0.2310\n",
      "  클러스터 3: 파워 셀러 (평균 매출: 270,055,591, 평균 주문: 3,936)\n",
      "  클러스터 2: 우수 셀러 (평균 매출: 218,778,003, 평균 주문: 2,930)\n",
      "  클러스터 0: 성장형 셀러 (평균 매출: 75,270,127, 평균 주문: 1,351)\n",
      "  클러스터 4: 관리 필요 셀러 (평균 매출: 48,288,975, 평균 주문: 767)\n",
      "  클러스터 1: 휴면 셀러 (평균 매출: 26,567,313, 평균 주문: 460)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model '셀러세그먼트' already exists. Creating a new version of this model...\n",
      "Created version '3' of model '셀러세그먼트'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.5] 매출 예측 모델 (LightGBM / GradientBoosting)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 21:52:58 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  알고리즘: LightGBM\n",
      "  MAE: 14,103,207, R2: 0.9593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model '매출예측' already exists. Creating a new version of this model...\n",
      "Created version '3' of model '매출예측'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.6] CS 응답 품질 모델 (RandomForest Classifier)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 21:53:11 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  정확도: 0.3950, F1(매크로): 0.3658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'CS응답품질' already exists. Creating a new version of this model...\n",
      "Created version '3' of model 'CS응답품질'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.7] 고객 LTV 예측 모델 (GradientBoosting Regressor)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 21:53:24 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MAE: 24,552, R2: 0.9611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model '고객LTV예측' already exists. Creating a new version of this model...\n",
      "Created version '3' of model '고객LTV예측'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.8] 리뷰 감성 분석 모델 (TF-IDF + LogisticRegression)\n",
      "  정확도: 1.0000, F1(매크로): 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 21:53:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "Registered model '리뷰감성분석' already exists. Creating a new version of this model...\n",
      "Created version '3' of model '리뷰감성분석'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.9] 상품 수요 예측 모델 (XGBoost / GradientBoosting)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 21:53:46 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  알고리즘: XGBoost\n",
      "  MAE: 2.54, R2: 0.8611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model '상품수요예측' already exists. Creating a new version of this model...\n",
      "Created version '3' of model '상품수요예측'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[3.10] 정산 이상 탐지 모델 (DBSCAN)\n",
      "  클러스터 수: 1, 이상(noise): 50건 (5.3%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 21:53:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2026/02/09 21:53:57 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "Registered model '정산이상탐지' already exists. Creating a new version of this model...\n",
      "Created version '3' of model '정산이상탐지'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PART 3 완료: 모든 모델 학습 완료 (10개)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "PART 4: 저장 및 테스트\n",
      "======================================================================\n",
      "\n",
      "[4.1] CSV 파일 저장 (17개)\n",
      "  18개 CSV 파일 저장 완료\n",
      "\n",
      "[4.2] 모델 파일 저장\n",
      "  - shap_explainer_churn.pkl 저장 완료\n",
      "  10개 모델 + 보조 파일 저장 완료\n",
      "\n",
      "[4.2.1] Guardian 감사 로그 IsolationForest 학습\n",
      "  guardian.db 시드 데이터 생성 완료 (audit_log: 320건)\n",
      "  학습 데이터: 320건, 7 features\n",
      "  이상 탐지: 15건 (4.7%)\n",
      "  model_guardian_anomaly.pkl, scaler_guardian.pkl 저장 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/09 21:54:09 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "Successfully registered model 'Guardian감사로그이상탐지'.\n",
      "Created version '1' of model 'Guardian감사로그이상탐지'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[4.3] 셀러별 모델 예측 결과 사전계산\n",
      "  매출 예측 완료: 300명\n",
      "  CS 품질 예측 완료: 300명\n",
      "  LTV 예측 완료: 300명\n",
      "  seller_analytics.csv 업데이트 완료 (예측 컬럼 추가)\n",
      "\n",
      "[4.3] 예측 함수 테스트\n",
      "  셀러 이탈 예측: 유지 (확률: 27.95%)\n",
      "  문의 분류: '배송이 너무 늦어요 언제 오나요?' -> 기술지원\n",
      "  감성 분석: '품질이 정말 좋고 배송도 빨라요! 추천합니다!' -> positive\n",
      "  셀러 세그먼트: 관리 필요 셀러\n",
      "\n",
      "======================================================================\n",
      "완료! 카페24 이커머스 데이터 생성 및 모델 학습 성공\n",
      "======================================================================\n",
      "\n",
      "[요약]\n",
      "  데이터:\n",
      "    - 쇼핑몰: 300개, 상품: 7285개, 셀러: 300명\n",
      "    - 운영 로그: 24720건, 서비스: 1163건\n",
      "    - 일별 지표: 90일, 셀러 활동: 27000건\n",
      "    - CSV 파일: 17개\n",
      "  모델 (11개):\n",
      "    1. 셀러 이탈 예측 (RandomForest + SHAP)\n",
      "    2. 이상거래 탐지 (Isolation Forest)\n",
      "    3. 문의 자동 분류 (TF-IDF + RandomForest)\n",
      "    4. 셀러 세그먼트 (K-Means)\n",
      "    5. 매출 예측 (LightGBM)\n",
      "    6. CS 응답 품질 (RandomForest)\n",
      "    7. 고객 LTV 예측 (GradientBoosting)\n",
      "    8. 리뷰 감성 분석 (TF-IDF + LogisticRegression)\n",
      "    9. 상품 수요 예측 (XGBoost)\n",
      "   10. 정산 이상 탐지 (DBSCAN)\n",
      "   11. Guardian 감사 로그 이상탐지 (IsolationForest, 320건)\n",
      "  SHAP: 활성화\n",
      "  MLflow: 활성화\n",
      "\n",
      "백엔드 서버 시작: cd \"backend 리팩토링 시작\" && python main.py\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "카페24 이커머스 AI 플랫폼 - 데이터 생성 및 모델 학습\n",
    "===================================================\n",
    "카페24 운영 AI 프로젝트\n",
    "\n",
    "구조:\n",
    "  PART 1: 설정 및 환경\n",
    "  PART 2: 데이터 생성 (18개 CSV)\n",
    "  PART 3: 모델 학습 (10개 ML 모델)\n",
    "  PART 4: 저장 및 테스트\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: 설정 및 환경\n",
    "# ============================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    RandomForestRegressor,\n",
    "    IsolationForest,\n",
    "    GradientBoostingRegressor,\n",
    "    GradientBoostingClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    silhouette_score,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "np.random.seed(42)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# MLflow 설정 (선택적)\n",
    "try:\n",
    "    import mlflow\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    MLFLOW_AVAILABLE = True\n",
    "    MLFLOW_TRACKING_URI = \"file:./mlruns\"\n",
    "    mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "    EXPERIMENT_NAME = \"cafe24-ops-ai\"\n",
    "    experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "    if experiment is None:\n",
    "        mlflow.create_experiment(EXPERIMENT_NAME)\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "    print(f\"MLflow Tracking URI: {MLFLOW_TRACKING_URI}\")\n",
    "    print(f\"MLflow Experiment: {EXPERIMENT_NAME}\")\n",
    "except ImportError:\n",
    "    MLFLOW_AVAILABLE = False\n",
    "    print(\"MLflow 미설치 - 실험 추적을 건너뜁니다\")\n",
    "\n",
    "# LightGBM (매출 예측용)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    LIGHTGBM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    LIGHTGBM_AVAILABLE = False\n",
    "    print(\"LightGBM 미설치 - GradientBoosting으로 대체합니다\")\n",
    "\n",
    "# XGBoost (수요 예측용)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost 미설치 - GradientBoosting으로 대체합니다\")\n",
    "\n",
    "# SHAP\n",
    "try:\n",
    "    import shap\n",
    "    SHAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SHAP_AVAILABLE = False\n",
    "    print(\"SHAP 미설치 - SHAP 분석을 건너뜁니다\")\n",
    "\n",
    "# 저장 경로\n",
    "try:\n",
    "    BACKEND_DIR = Path(__file__).parent.parent\n",
    "except NameError:\n",
    "    BACKEND_DIR = Path.cwd()\n",
    "    if BACKEND_DIR.name == \"ml\":\n",
    "        BACKEND_DIR = BACKEND_DIR.parent\n",
    "    elif \"backend\" in str(BACKEND_DIR).lower() or \"리팩토링\" in str(BACKEND_DIR):\n",
    "        pass\n",
    "    else:\n",
    "        BACKEND_DIR = Path.cwd()\n",
    "\n",
    "BACKEND_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PART 1: 설정 완료\")\n",
    "print(f\"  BACKEND_DIR: {BACKEND_DIR}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: 데이터 생성 (18개 CSV)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 2: 데이터 생성 (카페24 이커머스)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "reference_date = pd.to_datetime(\"2025-01-15\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 카페24 관련 상수 데이터\n",
    "# --------------------------------------------------------------------------\n",
    "PLAN_TIERS = [\"Basic\", \"Standard\", \"Premium\", \"Enterprise\"]\n",
    "PLAN_WEIGHTS = [0.35, 0.30, 0.25, 0.10]\n",
    "PLAN_TIER_ENCODE = {\"Basic\": 0, \"Standard\": 1, \"Premium\": 2, \"Enterprise\": 3}\n",
    "\n",
    "CATEGORIES_KO = [\"패션\", \"뷰티\", \"식품\", \"전자기기\", \"생활용품\", \"IT서비스\", \"교육\", \"스포츠\"]\n",
    "CATEGORIES_EN = [\"Fashion\", \"Beauty\", \"Food\", \"Electronics\", \"Household\", \"IT Services\", \"Education\", \"Sports\"]\n",
    "\n",
    "REGIONS = [\"서울\", \"경기\", \"인천\", \"부산\", \"대구\", \"대전\", \"광주\", \"제주\"]\n",
    "REGION_WEIGHTS = [0.30, 0.25, 0.12, 0.10, 0.07, 0.06, 0.05, 0.05]\n",
    "\n",
    "\n",
    "FASHION_PRODUCTS = [\n",
    "    \"캐시미어 니트\", \"오버핏 맨투맨\", \"와이드 슬랙스\", \"플리스 자켓\", \"데님 팬츠\",\n",
    "    \"롱 패딩\", \"가죽 백팩\", \"울 코트\", \"스트라이프 셔츠\", \"테일러드 블레이저\",\n",
    "    \"크로스백\", \"미니 스커트\", \"카고 팬츠\", \"린넨 원피스\", \"트렌치 코트\",\n",
    "]\n",
    "BEAUTY_PRODUCTS = [\n",
    "    \"수분 에센스\", \"선크림 SPF50+\", \"클렌징 오일\", \"립틴트\", \"쿠션 파운데이션\",\n",
    "    \"아이크림\", \"토너패드\", \"세럼 앰플\", \"마스크팩 세트\", \"헤어 에센스\",\n",
    "    \"비타민C 세럼\", \"각질제거 젤\", \"미스트 토너\", \"나이트 크림\", \"볼 블러셔\",\n",
    "]\n",
    "FOOD_PRODUCTS = [\n",
    "    \"프리미엄 한우 세트\", \"유기농 과일 박스\", \"수제 잼 세트\", \"건강 견과류\", \"전통 된장\",\n",
    "    \"올리브유 선물세트\", \"프로바이오틱스\", \"녹차 선물세트\", \"수제 초콜릿\", \"건과일 믹스\",\n",
    "    \"홍삼 추출액\", \"제주 감귤 박스\", \"참치캔 세트\", \"꿀 선물세트\", \"특등급 쌀\",\n",
    "]\n",
    "ELECTRONICS_PRODUCTS = [\n",
    "    \"블루투스 이어폰\", \"무선 충전기\", \"보조배터리\", \"스마트 워치\", \"USB 허브\",\n",
    "    \"LED 모니터\", \"기계식 키보드\", \"웹캠 HD\", \"마우스패드\", \"노트북 거치대\",\n",
    "    \"태블릿 케이스\", \"HDMI 케이블\", \"외장 SSD\", \"스피커 미니\", \"멀티탭\",\n",
    "]\n",
    "HOUSEHOLD_PRODUCTS = [\n",
    "    \"공기청정기 필터\", \"무선 청소기\", \"아로마 디퓨저\", \"텀블러 세트\", \"수건 선물세트\",\n",
    "    \"주방세제 세트\", \"쿠션 커버\", \"수납 박스\", \"LED 스탠드\", \"매트리스 토퍼\",\n",
    "    \"실리콘 주방도구\", \"빨래 건조대\", \"미니 가습기\", \"유리 밀폐용기\", \"방향제 세트\",\n",
    "]\n",
    "\n",
    "PRODUCT_MAP = {\n",
    "    \"패션\": FASHION_PRODUCTS,\n",
    "    \"뷰티\": BEAUTY_PRODUCTS,\n",
    "    \"식품\": FOOD_PRODUCTS,\n",
    "    \"전자기기\": ELECTRONICS_PRODUCTS,\n",
    "    \"생활용품\": HOUSEHOLD_PRODUCTS,\n",
    "}\n",
    "\n",
    "SHOP_NAME_PREFIXES = {\n",
    "    \"패션\": [\"스타일\", \"모드\", \"트렌드\", \"패션\", \"옷장\", \"룩\", \"데일리\"],\n",
    "    \"뷰티\": [\"글로우\", \"뷰티\", \"스킨\", \"코스\", \"피부\", \"에스테\"],\n",
    "    \"식품\": [\"맛있는\", \"신선한\", \"건강한\", \"프레시\", \"자연\", \"오가닉\"],\n",
    "    \"전자기기\": [\"테크\", \"디지\", \"스마트\", \"기가\", \"넥스트\", \"이노\"],\n",
    "    \"생활용품\": [\"홈\", \"리빙\", \"편리한\", \"깔끔한\", \"모던\", \"데코\"],\n",
    "    \"IT서비스\": [\"클라우드\", \"코드\", \"데이터\", \"AI\", \"넷\", \"웹\"],\n",
    "    \"교육\": [\"에듀\", \"러닝\", \"스쿨\", \"지식\", \"클래스\", \"배움\"],\n",
    "    \"스포츠\": [\"피트\", \"스포\", \"액티브\", \"런닝\", \"헬스\", \"짐\"],\n",
    "}\n",
    "SHOP_NAME_SUFFIXES = [\"마켓\", \"스토어\", \"몰\", \"샵\", \"하우스\", \"플러스\", \"랩\", \"존\"]\n",
    "\n",
    "CS_INQUIRY_TEMPLATES = {\n",
    "    \"배송\": [\n",
    "        \"택배사 연동 설정은 어디서 하나요? CJ대한통운 추가하고 싶습니다.\",\n",
    "        \"배송 추적 API 연동이 안 됩니다. 송장번호가 자동 입력되지 않아요.\",\n",
    "        \"해외배송 서비스 활성화 방법 알려주세요.\",\n",
    "        \"배송비 정책 설정에서 제주/도서산간 추가 배송비를 어떻게 설정하나요?\",\n",
    "        \"묶음배송 설정이 제대로 작동하지 않습니다.\",\n",
    "        \"배송 대행 서비스(풀필먼트) 신청 방법이 궁금합니다.\",\n",
    "        \"택배사별 운송장 출력이 안 됩니다. 프린터 연동 오류가 발생해요.\",\n",
    "        \"당일배송/새벽배송 옵션을 추가하고 싶은데 가능한가요?\",\n",
    "        \"배송비 조건부 무료 설정(5만원 이상 무료) 방법을 모르겠습니다.\",\n",
    "        \"해외 주문 건 통관 정보 입력은 어디서 하나요?\",\n",
    "    ],\n",
    "    \"환불\": [\n",
    "        \"고객 환불 처리 시 PG 수수료도 환불되나요?\",\n",
    "        \"부분 환불 처리 방법을 알려주세요. 관리자 페이지에서 어떻게 하나요?\",\n",
    "        \"반품 접수된 주문 건 자동 환불 설정이 가능한가요?\",\n",
    "        \"환불 처리했는데 고객에게 카드 취소가 며칠 걸리나요?\",\n",
    "        \"교환/반품 사유별 배송비 부담 설정은 어디서 하나요?\",\n",
    "        \"환불 정책을 쇼핑몰에 자동 표시하는 방법이 있나요?\",\n",
    "        \"PG사 정산 후 환불 건 처리 프로세스가 궁금합니다.\",\n",
    "        \"환불 대기 중인 주문 목록을 일괄 조회하고 싶습니다.\",\n",
    "        \"고객이 반품 요청했는데 관리자에서 승인 처리가 안 됩니다.\",\n",
    "        \"환불 완료된 건인데 고객이 입금 안 됐다고 합니다. 확인 부탁드립니다.\",\n",
    "    ],\n",
    "    \"결제\": [\n",
    "        \"PG사 연동 설정 중 오류가 발생합니다. 이니시스 키 등록이 안 돼요.\",\n",
    "        \"간편결제(카카오페이, 네이버페이) 추가 연동 방법을 알려주세요.\",\n",
    "        \"결제 테스트 모드에서 실제 결제로 전환하려면 어떻게 하나요?\",\n",
    "        \"해외 결제 수단(PayPal 등) 연동이 가능한가요?\",\n",
    "        \"결제 수수료율 확인 및 변경은 어디서 하나요?\",\n",
    "        \"가상계좌(무통장입금) 입금 확인이 자동으로 안 됩니다.\",\n",
    "        \"결제 오류가 자주 발생한다는 고객 문의가 많습니다. 원인이 뭔가요?\",\n",
    "        \"할부 결제 설정 방법과 PG사별 차이점이 궁금합니다.\",\n",
    "        \"정기결제(구독) 기능을 구현하고 싶은데 지원되나요?\",\n",
    "        \"세금계산서 자동 발행 설정 방법을 알려주세요.\",\n",
    "    ],\n",
    "    \"상품\": [\n",
    "        \"상품 대량 등록(엑셀 업로드)에서 오류가 발생합니다.\",\n",
    "        \"상품 옵션 조합 설정이 너무 복잡한데 쉬운 방법이 있나요?\",\n",
    "        \"카테고리별 상품 진열 순서를 변경하고 싶습니다.\",\n",
    "        \"상품 상세페이지 에디터에서 이미지가 깨집니다.\",\n",
    "        \"품절 상품 자동 숨김 처리 설정이 가능한가요?\",\n",
    "        \"상품 리뷰 관리에서 악성 리뷰를 일괄 삭제하는 방법이 있나요?\",\n",
    "        \"네이버 쇼핑 EP 연동 시 상품 정보가 누락됩니다.\",\n",
    "        \"상품 재고 관리 시 옵션별 재고 설정은 어디서 하나요?\",\n",
    "        \"타 플랫폼(쿠팡, 11번가)에서 상품을 가져오기 할 수 있나요?\",\n",
    "        \"상품 할인가 설정과 회원 등급별 할인 중복 적용이 가능한가요?\",\n",
    "    ],\n",
    "    \"계정\": [\n",
    "        \"쇼핑몰 관리자 비밀번호를 분실했습니다. 재설정 방법 알려주세요.\",\n",
    "        \"부운영자 계정을 추가하고 권한을 제한하고 싶습니다.\",\n",
    "        \"쇼핑몰 플랜(요금제) 업그레이드 방법과 차이점이 궁금합니다.\",\n",
    "        \"사업자 정보 변경(상호명, 대표자) 신청은 어떻게 하나요?\",\n",
    "        \"쇼핑몰 도메인을 변경하고 싶습니다. 기존 URL은 어떻게 되나요?\",\n",
    "        \"관리자 페이지 2단계 인증(OTP) 설정 방법을 알려주세요.\",\n",
    "        \"쇼핑몰 임시 폐쇄/휴면 처리 방법이 궁금합니다.\",\n",
    "        \"멀티쇼핑몰 기능으로 추가 쇼핑몰을 개설하고 싶습니다.\",\n",
    "        \"API 키 발급 방법과 사용 한도가 어떻게 되나요?\",\n",
    "        \"카페24 파트너 프로그램 가입 조건이 궁금합니다.\",\n",
    "    ],\n",
    "    \"정산\": [\n",
    "        \"이번 달 정산 금액이 예상보다 적습니다. 수수료 내역 확인 부탁드립니다.\",\n",
    "        \"정산 주기를 월 2회에서 주간 정산으로 변경할 수 있나요?\",\n",
    "        \"정산 보류된 건이 있는데 해제 방법을 알려주세요.\",\n",
    "        \"세금계산서와 정산 금액이 일치하지 않습니다.\",\n",
    "        \"해외 판매 건 정산 시 환율 적용 기준이 궁금합니다.\",\n",
    "        \"PG 수수료 외에 추가로 발생하는 비용이 있나요?\",\n",
    "        \"정산 내역 엑셀 다운로드가 안 됩니다.\",\n",
    "        \"카드 매출 정산과 계좌이체 정산 일정이 다른 이유가 뭔가요?\",\n",
    "        \"정산 계좌를 변경하고 싶습니다. 처리 기간이 어떻게 되나요?\",\n",
    "        \"부가세 신고용 자료를 어디서 다운로드할 수 있나요?\",\n",
    "    ],\n",
    "    \"기술지원\": [\n",
    "        \"카페24 API로 주문 목록을 가져오는데 인증 오류가 발생합니다.\",\n",
    "        \"쇼핑몰 디자인 스킨 수정 방법을 알려주세요. HTML/CSS를 직접 편집하고 싶습니다.\",\n",
    "        \"외부 스크립트(채널톡, GA4)를 쇼핑몰에 삽입하는 방법이 궁금합니다.\",\n",
    "        \"모바일 페이지 로딩 속도가 너무 느립니다. 최적화 방법이 있나요?\",\n",
    "        \"쇼핑몰에 커스텀 기능을 추가 개발 의뢰하고 싶습니다.\",\n",
    "        \"카페24 앱스토어에서 설치한 앱이 정상 작동하지 않습니다.\",\n",
    "        \"SSL 인증서 갱신이 자동으로 안 됩니다. 수동 갱신 방법 알려주세요.\",\n",
    "        \"REST API 호출 시 rate limit이 초과됐다는 오류가 나옵니다.\",\n",
    "        \"카페24 웹훅 설정 방법과 지원하는 이벤트 목록이 궁금합니다.\",\n",
    "        \"쇼핑몰 백업/복원 기능이 있나요? 데이터 이전이 필요합니다.\",\n",
    "    ],\n",
    "    \"마케팅\": [\n",
    "        \"네이버 쇼핑 광고 연동 설정 방법을 알려주세요.\",\n",
    "        \"카카오 픽셀 설치와 전환 추적 설정이 안 됩니다.\",\n",
    "        \"쿠폰 발급 시 특정 상품/카테고리만 적용하려면 어떻게 하나요?\",\n",
    "        \"회원 등급별 자동 할인 설정 방법이 궁금합니다.\",\n",
    "        \"Google Analytics 4(GA4) 연동은 어디서 하나요?\",\n",
    "        \"메타(페이스북) 픽셀 설치 방법을 알려주세요.\",\n",
    "        \"이메일 마케팅/뉴스레터 발송 기능이 있나요?\",\n",
    "        \"SEO 설정(메타 태그, 구조화 데이터) 방법이 궁금합니다.\",\n",
    "        \"타임세일/기획전 페이지를 만들고 싶은데 방법을 모르겠습니다.\",\n",
    "        \"적립금/포인트 정책을 변경하고 싶습니다. 기존 적립금에도 영향이 있나요?\",\n",
    "    ],\n",
    "    \"기타\": [\n",
    "        \"카페24 교육 프로그램이나 웨비나 일정이 궁금합니다.\",\n",
    "        \"쇼핑몰 운영 컨설팅 서비스를 받을 수 있나요?\",\n",
    "        \"카페24 서비스 장애 공지는 어디서 확인하나요?\",\n",
    "        \"타 플랫폼(고도몰, 메이크샵)에서 카페24로 이전하려면 어떻게 하나요?\",\n",
    "        \"카페24 파트너사/디자인 에이전시 추천 부탁드립니다.\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "REVIEW_TEMPLATES_POSITIVE = [\n",
    "    \"배송도 빠르고 상품 품질도 아주 좋습니다! 재구매 의사 있어요.\",\n",
    "    \"가격 대비 품질이 정말 훌륭합니다. 추천해요!\",\n",
    "    \"포장이 꼼꼼하고 상품 상태도 완벽했습니다.\",\n",
    "    \"사진과 동일하게 왔어요. 매우 만족합니다.\",\n",
    "    \"생각보다 훨씬 좋아서 놀랐습니다. 감사합니다!\",\n",
    "    \"색상도 예쁘고 사이즈도 딱 맞아요. 최고!\",\n",
    "    \"이 가격에 이 품질이면 정말 가성비 좋아요.\",\n",
    "    \"친구한테 선물했는데 너무 좋아했습니다.\",\n",
    "    \"배송 엄청 빨라요! 다음날 바로 도착했습니다.\",\n",
    "    \"재구매합니다. 이번에도 역시 좋네요.\",\n",
    "    \"두번째 구매인데 변함없는 품질에 만족합니다.\",\n",
    "    \"상세페이지 그대로 왔어요. 신뢰가 갑니다.\",\n",
    "    \"가족 모두 만족하고 있습니다. 감사해요!\",\n",
    "    \"소재가 정말 좋아요. 고급스러운 느낌입니다.\",\n",
    "    \"고객센터 응대도 친절하고 상품도 좋아요!\",\n",
    "]\n",
    "\n",
    "REVIEW_TEMPLATES_NEGATIVE = [\n",
    "    \"기대했는데 실망이에요. 사진과 너무 다릅니다.\",\n",
    "    \"배송이 너무 늦었습니다. 개선이 필요해요.\",\n",
    "    \"상품에 하자가 있어서 반품 진행 중입니다.\",\n",
    "    \"가격에 비해 품질이 떨어집니다.\",\n",
    "    \"포장이 엉성해서 상품이 파손되었습니다.\",\n",
    "    \"사이즈가 표기와 전혀 다릅니다.\",\n",
    "    \"냄새가 심해서 사용하기 어렵습니다.\",\n",
    "    \"색상이 사진과 완전히 다릅니다. 환불 원합니다.\",\n",
    "    \"한번 세탁했더니 형태가 변형되었습니다.\",\n",
    "    \"마감 처리가 매우 조잡합니다.\",\n",
    "    \"주문 후 일주일이 지났는데 배송 시작도 안 했어요.\",\n",
    "    \"고객센터 연락이 전혀 안 됩니다.\",\n",
    "    \"제품 설명에 없는 결함이 있었습니다.\",\n",
    "    \"가격 대비 매우 실망스러운 품질입니다.\",\n",
    "    \"두번째 교환인데도 불량이 왔어요.\",\n",
    "]\n",
    "\n",
    "REVIEW_TEMPLATES_NEUTRAL = [\n",
    "    \"보통이에요. 나쁘지는 않지만 특별하지도 않네요.\",\n",
    "    \"가격 생각하면 적당한 것 같습니다.\",\n",
    "    \"기대한 것과 비슷하게 왔습니다.\",\n",
    "    \"무난한 상품입니다. 평범해요.\",\n",
    "    \"배송은 빨랐는데 상품은 그저 그래요.\",\n",
    "    \"사용해봐야 알 것 같아요. 우선 보통입니다.\",\n",
    "    \"나쁘지 않은데 굳이 재구매는 안 할 것 같아요.\",\n",
    "    \"가격만큼의 가치는 있습니다.\",\n",
    "    \"포장은 좋았는데 상품은 보통이네요.\",\n",
    "    \"기능은 괜찮은데 디자인이 아쉬워요.\",\n",
    "]\n",
    "\n",
    "PLATFORM_DOCS_DATA = [\n",
    "    {\"doc_id\": \"DOC001\", \"title\": \"카페24 쇼핑몰 개설 가이드\", \"category\": \"시작하기\",\n",
    "     \"content_ko\": \"카페24에서 쇼핑몰을 개설하려면 사업자등록번호와 통신판매업 신고가 필요합니다. 기본 플랜부터 시작하여 매출 성장에 따라 업그레이드할 수 있습니다.\"},\n",
    "    {\"doc_id\": \"DOC002\", \"title\": \"결제 시스템 연동 방법\", \"category\": \"결제\",\n",
    "     \"content_ko\": \"카페24는 KG이니시스, 토스페이먼츠, NHN KCP 등 주요 PG사와 연동되어 있습니다. 관리자 페이지에서 결제 수단을 설정할 수 있습니다.\"},\n",
    "    {\"doc_id\": \"DOC003\", \"title\": \"배송 관리 정책\", \"category\": \"배송\",\n",
    "     \"content_ko\": \"주문 접수 후 2영업일 이내에 발송해야 합니다. 배송 추적번호는 자동으로 고객에게 발송됩니다.\"},\n",
    "    {\"doc_id\": \"DOC004\", \"title\": \"환불 및 교환 처리 가이드\", \"category\": \"CS\",\n",
    "     \"content_ko\": \"고객 환불 요청 시 7일 이내 처리가 원칙입니다. 상품 하자의 경우 판매자 부담으로 반품 배송비를 처리합니다.\"},\n",
    "    {\"doc_id\": \"DOC005\", \"title\": \"정산 시스템 안내\", \"category\": \"정산\",\n",
    "     \"content_ko\": \"정산은 매주 수요일에 진행됩니다. 결제 수수료는 플랜에 따라 2.5%~3.5%가 적용됩니다.\"},\n",
    "    {\"doc_id\": \"DOC006\", \"title\": \"마케팅 도구 활용 가이드\", \"category\": \"마케팅\",\n",
    "     \"content_ko\": \"카페24의 마케팅 도구를 활용하여 SEO 최적화, 이메일 마케팅, SNS 연동 등을 설정할 수 있습니다.\"},\n",
    "    {\"doc_id\": \"DOC007\", \"title\": \"상품 등록 및 관리\", \"category\": \"상품\",\n",
    "     \"content_ko\": \"상품 등록 시 카테고리, 가격, 재고, 옵션, 상세 설명을 입력합니다. 대량 등록은 엑셀 업로드로 가능합니다.\"},\n",
    "    {\"doc_id\": \"DOC008\", \"title\": \"쇼핑몰 디자인 커스터마이징\", \"category\": \"디자인\",\n",
    "     \"content_ko\": \"카페24는 다양한 무료/유료 디자인 템플릿을 제공합니다. HTML/CSS 수정으로 세부 디자인을 조정할 수 있습니다.\"},\n",
    "    {\"doc_id\": \"DOC009\", \"title\": \"고객 데이터 분석 가이드\", \"category\": \"분석\",\n",
    "     \"content_ko\": \"고객 행동 분석, 매출 리포트, 상품별 판매 현황 등을 대시보드에서 확인할 수 있습니다.\"},\n",
    "    {\"doc_id\": \"DOC010\", \"title\": \"API 연동 개발자 가이드\", \"category\": \"개발\",\n",
    "     \"content_ko\": \"카페24 Open API를 통해 주문, 상품, 고객 데이터에 접근할 수 있습니다. OAuth 2.0 인증을 사용합니다.\"},\n",
    "    {\"doc_id\": \"DOC011\", \"title\": \"플랜별 기능 비교\", \"category\": \"시작하기\",\n",
    "     \"content_ko\": \"Basic 플랜은 월 500건 주문 처리, Standard는 2000건, Premium은 무제한입니다. Enterprise는 전담 매니저를 제공합니다.\"},\n",
    "    {\"doc_id\": \"DOC012\", \"title\": \"해외 판매(글로벌 쇼핑몰) 가이드\", \"category\": \"글로벌\",\n",
    "     \"content_ko\": \"카페24 글로벌 서비스를 통해 다국어 쇼핑몰을 운영할 수 있습니다. 해외 배송 및 환율 자동 적용을 지원합니다.\"},\n",
    "]\n",
    "\n",
    "ECOMMERCE_GLOSSARY = [\n",
    "    {\"term_ko\": \"GMV\", \"term_en\": \"Gross Merchandise Value\", \"definition\": \"총 거래액. 플랫폼에서 발생한 전체 상품 판매 금액\", \"category\": \"매출\"},\n",
    "    {\"term_ko\": \"전환율\", \"term_en\": \"Conversion Rate\", \"definition\": \"방문자 중 실제 구매로 이어진 비율\", \"category\": \"마케팅\"},\n",
    "    {\"term_ko\": \"객단가\", \"term_en\": \"Average Order Value\", \"definition\": \"주문 1건당 평균 결제 금액\", \"category\": \"매출\"},\n",
    "    {\"term_ko\": \"재구매율\", \"term_en\": \"Repeat Purchase Rate\", \"definition\": \"기존 고객이 다시 구매하는 비율\", \"category\": \"고객\"},\n",
    "    {\"term_ko\": \"이탈률\", \"term_en\": \"Churn Rate\", \"definition\": \"일정 기간 내 서비스를 떠난 고객 또는 셀러의 비율\", \"category\": \"고객\"},\n",
    "    {\"term_ko\": \"LTV\", \"term_en\": \"Customer Lifetime Value\", \"definition\": \"고객 한 명이 전체 이용 기간 동안 발생시키는 총 매출\", \"category\": \"고객\"},\n",
    "    {\"term_ko\": \"CAC\", \"term_en\": \"Customer Acquisition Cost\", \"definition\": \"신규 고객 한 명을 획득하는 데 드는 비용\", \"category\": \"마케팅\"},\n",
    "    {\"term_ko\": \"정산\", \"term_en\": \"Settlement\", \"definition\": \"판매 대금에서 수수료를 차감한 후 셀러에게 지급하는 과정\", \"category\": \"결제\"},\n",
    "    {\"term_ko\": \"SKU\", \"term_en\": \"Stock Keeping Unit\", \"definition\": \"재고 관리 단위. 개별 상품을 식별하는 코드\", \"category\": \"상품\"},\n",
    "    {\"term_ko\": \"풀필먼트\", \"term_en\": \"Fulfillment\", \"definition\": \"주문 접수부터 배송 완료까지의 전체 물류 처리 과정\", \"category\": \"물류\"},\n",
    "    {\"term_ko\": \"PG\", \"term_en\": \"Payment Gateway\", \"definition\": \"온라인 결제를 중개하는 서비스 (이니시스, 토스페이먼츠 등)\", \"category\": \"결제\"},\n",
    "    {\"term_ko\": \"SEO\", \"term_en\": \"Search Engine Optimization\", \"definition\": \"검색 엔진 최적화. 검색 결과 상위 노출을 위한 전략\", \"category\": \"마케팅\"},\n",
    "    {\"term_ko\": \"RFM\", \"term_en\": \"Recency Frequency Monetary\", \"definition\": \"고객 세분화 기법. 최근 구매일, 구매 빈도, 구매 금액 기반\", \"category\": \"고객\"},\n",
    "    {\"term_ko\": \"ROAS\", \"term_en\": \"Return on Ad Spend\", \"definition\": \"광고비 대비 매출 비율\", \"category\": \"마케팅\"},\n",
    "    {\"term_ko\": \"크로스셀링\", \"term_en\": \"Cross-selling\", \"definition\": \"고객이 구매한 상품과 관련된 다른 상품을 추천하는 전략\", \"category\": \"마케팅\"},\n",
    "]\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.1 shops.csv (300개 쇼핑몰)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.1] 쇼핑몰 데이터 생성\")\n",
    "\n",
    "\n",
    "def generate_shop_name(category, idx):\n",
    "    prefixes = SHOP_NAME_PREFIXES.get(category, [\"마이\"])\n",
    "    prefix = rng.choice(prefixes)\n",
    "    suffix = rng.choice(SHOP_NAME_SUFFIXES)\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "\n",
    "shops_data = []\n",
    "for i in range(300):\n",
    "    shop_id = f\"S{i+1:04d}\"\n",
    "    category = rng.choice(CATEGORIES_KO, p=[0.20, 0.15, 0.15, 0.12, 0.12, 0.10, 0.08, 0.08])\n",
    "    region = rng.choice(REGIONS, p=REGION_WEIGHTS)\n",
    "    plan = rng.choice(PLAN_TIERS, p=PLAN_WEIGHTS)\n",
    "    open_date = reference_date - timedelta(days=int(rng.integers(30, 900)))\n",
    "\n",
    "    status_prob = rng.random()\n",
    "    if status_prob < 0.70:\n",
    "        status = \"active\"\n",
    "    elif status_prob < 0.88:\n",
    "        status = \"dormant\"\n",
    "    else:\n",
    "        status = \"churned\"\n",
    "\n",
    "    shop_name = generate_shop_name(category, i)\n",
    "    desc = f\"{category} 전문 온라인 쇼핑몰. {region} 기반으로 다양한 {category} 상품을 판매합니다.\"\n",
    "    shops_data.append({\n",
    "        \"shop_id\": shop_id, \"shop_name\": shop_name, \"plan_tier\": plan,\n",
    "        \"category\": category, \"region\": region,\n",
    "        \"open_date\": open_date.strftime(\"%Y-%m-%d\"), \"description\": desc, \"status\": status,\n",
    "    })\n",
    "\n",
    "shops_df = pd.DataFrame(shops_data)\n",
    "print(f\"  - 쇼핑몰: {len(shops_df)}개 (active: {(shops_df['status']=='active').sum()}, \"\n",
    "      f\"dormant: {(shops_df['status']=='dormant').sum()}, churned: {(shops_df['status']=='churned').sum()})\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.2 categories.csv (8개 카테고리)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.2] 카테고리 데이터 생성\")\n",
    "\n",
    "categories_data = []\n",
    "for idx, (ko, en) in enumerate(zip(CATEGORIES_KO, CATEGORIES_EN)):\n",
    "    categories_data.append({\n",
    "        \"cat_id\": f\"CAT{idx+1:03d}\", \"name_ko\": ko, \"name_en\": en,\n",
    "        \"parent_cat\": \"ROOT\",\n",
    "        \"description_ko\": f\"{ko} 관련 상품을 판매하는 카테고리입니다.\",\n",
    "        \"description_en\": f\"Category for {en} related products.\",\n",
    "    })\n",
    "categories_df = pd.DataFrame(categories_data)\n",
    "print(f\"  - 카테고리: {len(categories_df)}개\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.3 services.csv (쇼핑몰별 서비스)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.3] 서비스 데이터 생성\")\n",
    "\n",
    "SERVICE_TYPES = [\"hosting\", \"payment\", \"shipping\", \"marketing\"]\n",
    "SERVICE_NAMES = {\n",
    "    \"hosting\": [\"웹호스팅 기본\", \"클라우드 서버\", \"CDN 가속\", \"SSL 인증서\"],\n",
    "    \"payment\": [\"카드결제\", \"간편결제(카카오페이)\", \"간편결제(네이버페이)\", \"가상계좌\"],\n",
    "    \"shipping\": [\"CJ대한통운\", \"한진택배\", \"롯데택배\", \"우체국택배\"],\n",
    "    \"marketing\": [\"SEO 최적화\", \"이메일 마케팅\", \"SNS 연동\", \"키워드 광고\"],\n",
    "}\n",
    "\n",
    "services_data = []\n",
    "svc_idx = 0\n",
    "for _, shop in shops_df.iterrows():\n",
    "    n_services = rng.integers(2, 7)\n",
    "    for _ in range(n_services):\n",
    "        svc_type = rng.choice(SERVICE_TYPES)\n",
    "        svc_name = rng.choice(SERVICE_NAMES[svc_type])\n",
    "        svc_status = \"active\" if shop[\"status\"] == \"active\" and rng.random() < 0.85 else \"inactive\"\n",
    "        svc_idx += 1\n",
    "        services_data.append({\n",
    "            \"service_id\": f\"SVC{svc_idx:05d}\", \"shop_id\": shop[\"shop_id\"],\n",
    "            \"service_name\": svc_name, \"service_type\": svc_type,\n",
    "            \"status\": svc_status,\n",
    "            \"description\": f\"{shop['shop_name']}의 {svc_name} 서비스\",\n",
    "        })\n",
    "\n",
    "services_df = pd.DataFrame(services_data)\n",
    "print(f\"  - 서비스: {len(services_df)}건\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.4 products.csv (2000+ 상품)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.4] 상품 데이터 생성\")\n",
    "\n",
    "products_data = []\n",
    "prod_idx = 0\n",
    "for _, shop in shops_df.iterrows():\n",
    "    category = shop[\"category\"]\n",
    "    n_products = rng.integers(10, 40)\n",
    "    product_names_pool = PRODUCT_MAP.get(category, HOUSEHOLD_PRODUCTS)\n",
    "    for j in range(n_products):\n",
    "        prod_idx += 1\n",
    "        pname = rng.choice(product_names_pool) + f\" {rng.choice(['A', 'B', 'C', 'S', 'X', 'Pro', 'Lite', ''])}{rng.integers(1, 99)}\"\n",
    "        price_base = {\n",
    "            \"패션\": 35000, \"뷰티\": 22000, \"식품\": 28000, \"전자기기\": 45000,\n",
    "            \"생활용품\": 18000, \"IT서비스\": 50000, \"교육\": 30000, \"스포츠\": 40000,\n",
    "        }.get(category, 25000)\n",
    "        price = int(np.clip(rng.lognormal(np.log(price_base), 0.5), 3000, 500000))\n",
    "        price = round(price, -2)\n",
    "\n",
    "        status_r = rng.random()\n",
    "        if status_r < 0.75:\n",
    "            p_status = \"active\"\n",
    "        elif status_r < 0.90:\n",
    "            p_status = \"sold_out\"\n",
    "        else:\n",
    "            p_status = \"hidden\"\n",
    "\n",
    "        listed_date = reference_date - timedelta(days=int(rng.integers(1, 365)))\n",
    "        stock = int(rng.integers(0, 500)) if p_status != \"sold_out\" else 0\n",
    "\n",
    "        products_data.append({\n",
    "            \"product_id\": f\"P{prod_idx:05d}\", \"shop_id\": shop[\"shop_id\"],\n",
    "            \"product_name\": pname.strip(), \"price\": price, \"category\": category,\n",
    "            \"status\": p_status, \"listed_date\": listed_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"stock_qty\": stock,\n",
    "        })\n",
    "\n",
    "products_df = pd.DataFrame(products_data)\n",
    "print(f\"  - 상품: {len(products_df)}개\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.5 sellers.csv (300명 셀러, 쇼핑몰과 1:1 매칭)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.5] 셀러 데이터 생성\")\n",
    "\n",
    "sellers_data = []\n",
    "for _, shop in shops_df.iterrows():\n",
    "    sid = shop[\"shop_id\"].replace(\"S\", \"SEL\")\n",
    "    join_date = pd.to_datetime(shop[\"open_date\"])\n",
    "    days_since_join = (reference_date - join_date).days\n",
    "\n",
    "    if shop[\"status\"] == \"churned\":\n",
    "        last_login = join_date + timedelta(days=int(rng.integers(1, max(2, days_since_join - 30))))\n",
    "        total_orders = int(rng.integers(5, 200))\n",
    "        total_revenue = int(total_orders * rng.integers(15000, 80000))\n",
    "    elif shop[\"status\"] == \"dormant\":\n",
    "        last_login = reference_date - timedelta(days=int(rng.integers(14, 60)))\n",
    "        total_orders = int(rng.integers(50, 800))\n",
    "        total_revenue = int(total_orders * rng.integers(20000, 90000))\n",
    "    else:\n",
    "        last_login = reference_date - timedelta(days=int(rng.integers(0, 7)))\n",
    "        total_orders = int(rng.integers(100, 5000))\n",
    "        total_revenue = int(total_orders * rng.integers(25000, 120000))\n",
    "\n",
    "    product_count = len(products_df[products_df[\"shop_id\"] == shop[\"shop_id\"]])\n",
    "    sellers_data.append({\n",
    "        \"seller_id\": sid,\n",
    "        \"plan_tier\": shop[\"plan_tier\"], \"region\": shop[\"region\"],\n",
    "        \"join_date\": join_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"last_login\": last_login.strftime(\"%Y-%m-%d\"),\n",
    "        \"status\": shop[\"status\"],\n",
    "        \"total_orders\": total_orders, \"total_revenue\": total_revenue,\n",
    "        \"product_count\": product_count,\n",
    "    })\n",
    "\n",
    "sellers_df = pd.DataFrame(sellers_data)\n",
    "print(f\"  - 셀러: {len(sellers_df)}명\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.6 operation_logs.csv (최대 30,000행)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.6] 운영 로그 데이터 생성\")\n",
    "\n",
    "OP_EVENT_TYPES = [\n",
    "    \"order_received\", \"product_listed\", \"cs_ticket\", \"payment_settled\",\n",
    "    \"refund_processed\", \"marketing_campaign\", \"product_updated\", \"login\",\n",
    "]\n",
    "OP_EVENT_WEIGHTS = [0.25, 0.12, 0.15, 0.13, 0.08, 0.07, 0.10, 0.10]\n",
    "\n",
    "op_logs_data = []\n",
    "log_idx = 0\n",
    "target_logs = 30000\n",
    "logs_per_seller = target_logs // len(sellers_df)\n",
    "\n",
    "for _, seller in sellers_df.iterrows():\n",
    "    n_logs = min(logs_per_seller + rng.integers(-50, 50), 500)\n",
    "    if seller[\"status\"] == \"churned\":\n",
    "        n_logs = max(20, n_logs // 4)\n",
    "    elif seller[\"status\"] == \"dormant\":\n",
    "        n_logs = max(50, n_logs // 2)\n",
    "\n",
    "    join_d = pd.to_datetime(seller[\"join_date\"])\n",
    "    last_d = pd.to_datetime(seller[\"last_login\"])\n",
    "    active_span = max(1, (last_d - join_d).days)\n",
    "\n",
    "    for _ in range(n_logs):\n",
    "        log_idx += 1\n",
    "        evt = rng.choice(OP_EVENT_TYPES, p=OP_EVENT_WEIGHTS)\n",
    "        evt_offset = int(rng.integers(0, active_span))\n",
    "        evt_date = join_d + timedelta(\n",
    "            days=evt_offset,\n",
    "            hours=int(rng.integers(0, 24)),\n",
    "            minutes=int(rng.integers(0, 60)),\n",
    "        )\n",
    "\n",
    "        if evt == \"order_received\":\n",
    "            detail = json.dumps({\n",
    "                \"order_amount\": int(rng.integers(10000, 200000)),\n",
    "                \"payment_method\": rng.choice([\"card\", \"kakao\", \"naver\", \"bank\"]),\n",
    "            }, ensure_ascii=False)\n",
    "        elif evt == \"cs_ticket\":\n",
    "            detail = json.dumps({\n",
    "                \"category\": rng.choice([\"배송\", \"환불\", \"결제\", \"상품\", \"계정\"]),\n",
    "                \"priority\": rng.choice([\"urgent\", \"high\", \"normal\", \"low\"]),\n",
    "            }, ensure_ascii=False)\n",
    "        elif evt == \"refund_processed\":\n",
    "            detail = json.dumps({\n",
    "                \"refund_amount\": int(rng.integers(5000, 150000)),\n",
    "                \"reason\": rng.choice([\"상품 불량\", \"고객 변심\", \"배송 오류\", \"기타\"]),\n",
    "            }, ensure_ascii=False)\n",
    "        else:\n",
    "            detail = \"{}\"\n",
    "\n",
    "        op_logs_data.append({\n",
    "            \"log_id\": f\"LOG{log_idx:07d}\", \"seller_id\": seller[\"seller_id\"],\n",
    "            \"event_type\": evt, \"event_date\": evt_date.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"details_json\": detail,\n",
    "        })\n",
    "        if log_idx >= target_logs:\n",
    "            break\n",
    "    if log_idx >= target_logs:\n",
    "        break\n",
    "\n",
    "op_logs_df = pd.DataFrame(op_logs_data)\n",
    "print(f\"  - 운영 로그: {len(op_logs_df)}건\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.7 seller_analytics.csv (300명)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.7] 셀러 분석 데이터 생성\")\n",
    "\n",
    "seller_analytics_data = []\n",
    "for _, seller in sellers_df.iterrows():\n",
    "    days_since_reg = (reference_date - pd.to_datetime(seller[\"join_date\"])).days\n",
    "    days_since_login = (reference_date - pd.to_datetime(seller[\"last_login\"])).days\n",
    "    plan_encoded = PLAN_TIER_ENCODE.get(seller[\"plan_tier\"], 0)\n",
    "\n",
    "    cs_tickets = int(rng.integers(0, max(1, seller[\"total_orders\"] // 5)))\n",
    "    refund_rate = round(float(np.clip(\n",
    "        rng.beta(2, 20) if seller[\"status\"] == \"active\" else rng.beta(3, 10), 0.0, 0.5\n",
    "    )), 4)\n",
    "    avg_response_time = round(float(np.clip(rng.exponential(4) + 0.5, 0.5, 72)), 1)\n",
    "\n",
    "    seller_analytics_data.append({\n",
    "        \"seller_id\": seller[\"seller_id\"],\n",
    "        \"total_orders\": seller[\"total_orders\"],\n",
    "        \"total_revenue\": seller[\"total_revenue\"],\n",
    "        \"product_count\": seller[\"product_count\"],\n",
    "        \"cs_tickets\": cs_tickets,\n",
    "        \"refund_rate\": refund_rate,\n",
    "        \"avg_response_time\": avg_response_time,\n",
    "        \"days_since_last_login\": days_since_login,\n",
    "        \"days_since_register\": days_since_reg,\n",
    "        \"plan_tier_encoded\": plan_encoded,\n",
    "        \"cluster\": -1,\n",
    "        \"churn_risk\": -1,\n",
    "        \"churn_probability\": -1.0,\n",
    "    })\n",
    "\n",
    "seller_analytics_df = pd.DataFrame(seller_analytics_data)\n",
    "print(f\"  - 셀러 분석: {len(seller_analytics_df)}명\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.8 shop_performance.csv (300개 쇼핑몰)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.8] 쇼핑몰 성과 데이터 생성\")\n",
    "\n",
    "shop_perf_data = []\n",
    "for _, shop in shops_df.iterrows():\n",
    "    if shop[\"status\"] == \"active\":\n",
    "        conv = round(float(rng.uniform(1.5, 6.0)), 2)\n",
    "        rev = int(rng.lognormal(15, 0.8))\n",
    "        orders = int(rev / rng.integers(25000, 80000))\n",
    "        visitors = int(orders / (conv / 100))\n",
    "    elif shop[\"status\"] == \"dormant\":\n",
    "        conv = round(float(rng.uniform(0.5, 2.5)), 2)\n",
    "        rev = int(rng.lognormal(13, 0.7))\n",
    "        orders = int(rev / rng.integers(20000, 60000))\n",
    "        visitors = int(orders / max(0.005, conv / 100))\n",
    "    else:\n",
    "        conv = round(float(rng.uniform(0.1, 1.0)), 2)\n",
    "        rev = int(rng.lognormal(11, 0.6))\n",
    "        orders = max(0, int(rev / rng.integers(15000, 50000)))\n",
    "        visitors = max(10, int(orders / max(0.001, conv / 100)))\n",
    "\n",
    "    shop_perf_data.append({\n",
    "        \"shop_id\": shop[\"shop_id\"],\n",
    "        \"conversion_rate\": conv,\n",
    "        \"avg_order_value\": int(rng.integers(15000, 120000)),\n",
    "        \"return_rate\": round(float(rng.beta(2, 20)), 4),\n",
    "        \"review_score\": round(float(np.clip(rng.normal(4.2, 0.5), 1.0, 5.0)), 2),\n",
    "        \"monthly_revenue\": rev,\n",
    "        \"monthly_orders\": orders,\n",
    "        \"monthly_visitors\": visitors,\n",
    "        \"customer_retention_rate\": round(float(np.clip(rng.beta(5, 3), 0.1, 0.95)), 4),\n",
    "    })\n",
    "\n",
    "shop_perf_df = pd.DataFrame(shop_perf_data)\n",
    "print(f\"  - 쇼핑몰 성과: {len(shop_perf_df)}개\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.9 daily_metrics.csv (90일)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.9] 일별 플랫폼 지표 생성\")\n",
    "\n",
    "daily_metrics_data = []\n",
    "base_active = 72\n",
    "base_gmv = 85000000\n",
    "for day in range(90):\n",
    "    d = reference_date - timedelta(days=89 - day)\n",
    "    weekend = 1.12 if d.weekday() >= 5 else 1.0\n",
    "    active = int(base_active * weekend * rng.uniform(0.90, 1.10))\n",
    "    gmv = int(base_gmv * weekend * rng.uniform(0.85, 1.20))\n",
    "    total_orders = int(gmv / rng.integers(30000, 70000))\n",
    "    daily_metrics_data.append({\n",
    "        \"date\": d.strftime(\"%Y-%m-%d\"),\n",
    "        \"active_shops\": active,\n",
    "        \"total_gmv\": gmv,\n",
    "        \"new_signups\": int(rng.integers(0, 5)),\n",
    "        \"total_orders\": total_orders,\n",
    "        \"avg_settlement_time\": round(float(rng.uniform(1.5, 4.5)), 1),\n",
    "        \"cs_tickets_open\": int(rng.integers(20, 80)),\n",
    "        \"cs_tickets_resolved\": int(rng.integers(15, 75)),\n",
    "        \"fraud_alerts\": int(rng.integers(0, 6)),\n",
    "        \"total_sessions\": int(active * rng.uniform(2.5, 4.5)),\n",
    "        \"avg_session_minutes\": round(float(rng.uniform(12, 35)), 1),\n",
    "    })\n",
    "\n",
    "daily_metrics_df = pd.DataFrame(daily_metrics_data)\n",
    "print(f\"  - 일별 지표: {len(daily_metrics_df)}일\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.10 cs_stats.csv\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.10] CS 통계 데이터 생성\")\n",
    "\n",
    "CS_CATEGORIES = [\"배송\", \"환불\", \"결제\", \"상품\", \"계정\", \"정산\", \"기술지원\", \"마케팅\", \"기타\"]\n",
    "cs_stats_data = []\n",
    "for cat in CS_CATEGORIES:\n",
    "    cs_stats_data.append({\n",
    "        \"category\": cat,\n",
    "        \"total_tickets\": int(rng.integers(50, 500)),\n",
    "        \"avg_resolution_hours\": round(float(rng.uniform(1, 48)), 1),\n",
    "        \"satisfaction_score\": round(float(np.clip(rng.normal(3.8, 0.6), 1.0, 5.0)), 2),\n",
    "    })\n",
    "cs_stats_df = pd.DataFrame(cs_stats_data)\n",
    "print(f\"  - CS 통계: {len(cs_stats_df)}개 카테고리\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.11 fraud_details.csv\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.11] 이상거래 상세 데이터 생성\")\n",
    "\n",
    "ANOMALY_TYPES = [\"high_refund\", \"fake_review\", \"price_manipulation\", \"unusual_volume\"]\n",
    "fraud_data = []\n",
    "anomaly_sellers = rng.choice(\n",
    "    sellers_df[\"seller_id\"].values, size=min(15, len(sellers_df)), replace=False\n",
    ")\n",
    "for sid in anomaly_sellers:\n",
    "    fraud_data.append({\n",
    "        \"seller_id\": sid,\n",
    "        \"anomaly_score\": round(float(rng.uniform(0.6, 1.0)), 4),\n",
    "        \"anomaly_type\": rng.choice(ANOMALY_TYPES),\n",
    "        \"detected_date\": (reference_date - timedelta(days=int(rng.integers(0, 30)))).strftime(\"%Y-%m-%d\"),\n",
    "        \"details\": f\"이상 패턴 감지: {rng.choice(['환불율 급증', '비정상 리뷰 패턴', '가격 이상 변동', '주문량 급변'])}\",\n",
    "    })\n",
    "fraud_df = pd.DataFrame(fraud_data)\n",
    "print(f\"  - 이상거래 상세: {len(fraud_df)}건\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.12 cohort_retention.csv\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.12] 코호트 리텐션 데이터 생성\")\n",
    "\n",
    "cohort_data = []\n",
    "cohort_months = [\"2024-07\", \"2024-08\", \"2024-09\", \"2024-10\", \"2024-11\", \"2024-12\"]\n",
    "for m in cohort_months:\n",
    "    w1 = round(float(np.clip(rng.normal(85, 5), 60, 98)), 1)\n",
    "    w2 = round(float(np.clip(w1 * rng.uniform(0.75, 0.90), 40, 95)), 1)\n",
    "    w4 = round(float(np.clip(w2 * rng.uniform(0.65, 0.85), 25, 85)), 1)\n",
    "    w8 = round(float(np.clip(w4 * rng.uniform(0.60, 0.80), 15, 70)), 1)\n",
    "    w12 = round(float(np.clip(w8 * rng.uniform(0.55, 0.75), 8, 55)), 1)\n",
    "    cohort_data.append({\n",
    "        \"cohort_month\": m, \"week1\": w1, \"week2\": w2,\n",
    "        \"week4\": w4, \"week8\": w8, \"week12\": w12,\n",
    "    })\n",
    "cohort_df = pd.DataFrame(cohort_data)\n",
    "print(f\"  - 코호트 리텐션: {len(cohort_df)}개월\")\n",
    "\n",
    "# 전환 퍼널 데이터 (코호트별)\n",
    "conversion_data = []\n",
    "for m in cohort_months:\n",
    "    registered = int(rng.integers(80, 150))\n",
    "    activated = int(registered * rng.uniform(0.60, 0.85))\n",
    "    engaged = int(activated * rng.uniform(0.50, 0.75))\n",
    "    converted = int(engaged * rng.uniform(0.30, 0.60))\n",
    "    retained = int(converted * rng.uniform(0.40, 0.70))\n",
    "    conversion_data.append({\n",
    "        \"cohort\": m, \"registered\": registered, \"activated\": activated,\n",
    "        \"engaged\": engaged, \"converted\": converted, \"retained\": retained,\n",
    "    })\n",
    "conversion_df = pd.DataFrame(conversion_data)\n",
    "print(f\"  - 전환 퍼널: {len(conversion_df)}개월\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.13 seller_activity.csv (300 셀러 x 90일)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.13] 셀러 일별 활동 데이터 생성\")\n",
    "\n",
    "seller_activity_data = []\n",
    "for _, seller in sellers_df.iterrows():\n",
    "    seller_last = pd.to_datetime(seller[\"last_login\"])\n",
    "    for day in range(90):\n",
    "        d = reference_date - timedelta(days=89 - day)\n",
    "        if d > seller_last and seller[\"status\"] in (\"dormant\", \"churned\"):\n",
    "            orders_p = 0\n",
    "            products_u = 0\n",
    "            cs_h = 0\n",
    "            revenue = 0\n",
    "        else:\n",
    "            activity_mult = {\n",
    "                \"active\": 1.0, \"dormant\": 0.3, \"churned\": 0.05,\n",
    "            }.get(seller[\"status\"], 0.5)\n",
    "            orders_p = int(np.clip(rng.poisson(5 * activity_mult), 0, 50))\n",
    "            products_u = int(np.clip(rng.poisson(3 * activity_mult), 0, 20))\n",
    "            cs_h = int(rng.integers(0, max(1, int(4 * activity_mult))))\n",
    "            revenue = int(orders_p * rng.integers(15000, 80000)) if orders_p > 0 else 0\n",
    "\n",
    "        seller_activity_data.append({\n",
    "            \"seller_id\": seller[\"seller_id\"], \"date\": d.strftime(\"%Y-%m-%d\"),\n",
    "            \"orders_processed\": orders_p, \"products_updated\": products_u,\n",
    "            \"cs_handled\": cs_h, \"revenue\": revenue,\n",
    "        })\n",
    "\n",
    "seller_activity_df = pd.DataFrame(seller_activity_data)\n",
    "print(f\"  - 셀러 일별 활동: {len(seller_activity_df)}건\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.14 platform_docs.csv\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.14] 플랫폼 문서 데이터 생성\")\n",
    "\n",
    "platform_docs_df = pd.DataFrame(PLATFORM_DOCS_DATA)\n",
    "print(f\"  - 플랫폼 문서: {len(platform_docs_df)}건\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.15 ecommerce_glossary.csv\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.15] 이커머스 용어 사전 생성\")\n",
    "\n",
    "glossary_df = pd.DataFrame(ECOMMERCE_GLOSSARY)\n",
    "print(f\"  - 용어 사전: {len(glossary_df)}건\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.16 seller_products.csv\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.16] 셀러-상품 매핑 데이터 생성\")\n",
    "\n",
    "seller_products_data = []\n",
    "for _, seller in sellers_df.iterrows():\n",
    "    shop_id = seller[\"seller_id\"].replace(\"SEL\", \"S\")\n",
    "    shop_products = products_df[products_df[\"shop_id\"] == shop_id]\n",
    "    for _, prod in shop_products.iterrows():\n",
    "        seller_products_data.append({\n",
    "            \"seller_id\": seller[\"seller_id\"],\n",
    "            \"product_id\": prod[\"product_id\"],\n",
    "            \"stock_qty\": prod[\"stock_qty\"],\n",
    "            \"price\": prod[\"price\"],\n",
    "            \"category\": prod[\"category\"],\n",
    "            \"status\": prod[\"status\"],\n",
    "        })\n",
    "\n",
    "seller_products_df = pd.DataFrame(seller_products_data)\n",
    "print(f\"  - 셀러 상품: {len(seller_products_df)}건\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2.17 seller_resources.csv\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[2.17] 셀러 리소스 데이터 생성\")\n",
    "\n",
    "PLAN_QUOTA = {\"Basic\": 5, \"Standard\": 20, \"Premium\": 50, \"Enterprise\": 200}\n",
    "seller_resources_data = []\n",
    "for _, seller in sellers_df.iterrows():\n",
    "    quota = PLAN_QUOTA.get(seller[\"plan_tier\"], 10)\n",
    "    used = round(float(np.clip(rng.uniform(0.1, 1.0) * quota, 0.1, quota * 0.95)), 2)\n",
    "    seller_resources_data.append({\n",
    "        \"seller_id\": seller[\"seller_id\"],\n",
    "        \"plan_quota_gb\": quota,\n",
    "        \"storage_used_gb\": used,\n",
    "        \"api_calls_monthly\": int(rng.integers(100, 50000)),\n",
    "        \"marketing_budget\": int(rng.integers(0, 5000000)),\n",
    "        \"ad_spend\": int(rng.integers(0, 3000000)),\n",
    "    })\n",
    "\n",
    "seller_resources_df = pd.DataFrame(seller_resources_data)\n",
    "print(f\"  - 셀러 리소스: {len(seller_resources_df)}건\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 2 완료: 모든 데이터 생성 완료 (18개 CSV)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: 모델 학습 (10개 ML 모델)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 3: 모델 학습 (10개)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.1 셀러 이탈 예측 (RandomForest + SHAP)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[3.1] 셀러 이탈 예측 모델 (RandomForest + SHAP)\")\n",
    "\n",
    "CHURN_FEATURES = [\n",
    "    \"total_orders\", \"total_revenue\", \"product_count\", \"cs_tickets\",\n",
    "    \"refund_rate\", \"avg_response_time\", \"days_since_last_login\",\n",
    "    \"days_since_register\", \"plan_tier_encoded\",\n",
    "]\n",
    "CHURN_FEATURE_NAMES_KR = {\n",
    "    \"total_orders\": \"총 주문수\", \"total_revenue\": \"총 매출\",\n",
    "    \"product_count\": \"상품 수\", \"cs_tickets\": \"CS 문의 수\",\n",
    "    \"refund_rate\": \"환불률\", \"avg_response_time\": \"평균 응답 시간\",\n",
    "    \"days_since_last_login\": \"마지막 로그인 경과일\",\n",
    "    \"days_since_register\": \"가입 후 경과일\",\n",
    "    \"plan_tier_encoded\": \"플랜 등급\",\n",
    "}\n",
    "\n",
    "is_churned = (sellers_df[\"status\"] == \"churned\").astype(int).values\n",
    "seller_analytics_df[\"is_churned\"] = is_churned\n",
    "\n",
    "X_churn = seller_analytics_df[CHURN_FEATURES].fillna(0).copy()\n",
    "y_churn = seller_analytics_df[\"is_churned\"].copy()\n",
    "\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
    "    X_churn, y_churn, test_size=0.2, random_state=42, stratify=y_churn,\n",
    ")\n",
    "\n",
    "churn_params = {\n",
    "    \"n_estimators\": 200, \"max_depth\": 8, \"min_samples_split\": 5,\n",
    "    \"min_samples_leaf\": 2, \"class_weight\": \"balanced\", \"random_state\": 42,\n",
    "}\n",
    "rf_churn = RandomForestClassifier(**churn_params, n_jobs=-1)\n",
    "rf_churn.fit(X_train_c, y_train_c)\n",
    "\n",
    "y_pred_c = rf_churn.predict(X_test_c)\n",
    "acc_churn = accuracy_score(y_test_c, y_pred_c)\n",
    "f1_churn = f1_score(y_test_c, y_pred_c, zero_division=0)\n",
    "print(f\"  정확도: {acc_churn:.4f}, F1: {f1_churn:.4f}\")\n",
    "\n",
    "feature_importances_churn = dict(zip(CHURN_FEATURES, rf_churn.feature_importances_))\n",
    "\n",
    "# SHAP\n",
    "shap_explainer = None\n",
    "if SHAP_AVAILABLE:\n",
    "    try:\n",
    "        shap_explainer = shap.TreeExplainer(rf_churn)\n",
    "        shap_values_raw = shap_explainer.shap_values(X_churn)\n",
    "        if isinstance(shap_values_raw, list) and len(shap_values_raw) == 2:\n",
    "            shap_vals = shap_values_raw[1]\n",
    "        elif hasattr(shap_values_raw, \"values\"):\n",
    "            shap_vals = shap_values_raw.values\n",
    "        elif isinstance(shap_values_raw, np.ndarray) and shap_values_raw.ndim == 3:\n",
    "            shap_vals = shap_values_raw[:, :, 1]\n",
    "        else:\n",
    "            shap_vals = np.array(shap_values_raw)\n",
    "        for i, feat in enumerate(CHURN_FEATURES):\n",
    "            seller_analytics_df[f\"shap_{feat}\"] = shap_vals[:, i]\n",
    "        print(\"  SHAP 분석 완료\")\n",
    "    except Exception as e:\n",
    "        print(f\"  SHAP 분석 오류: {e}\")\n",
    "        SHAP_AVAILABLE = False\n",
    "\n",
    "# 이탈 확률 기록\n",
    "churn_proba = rf_churn.predict_proba(X_churn)[:, 1]\n",
    "seller_analytics_df[\"churn_probability\"] = np.round(churn_proba, 4)\n",
    "seller_analytics_df[\"churn_risk\"] = (churn_proba >= 0.5).astype(int)\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    with mlflow.start_run(run_name=\"seller_churn_model\"):\n",
    "        mlflow.set_tag(\"model_type\", \"classification\")\n",
    "        mlflow.log_params(churn_params)\n",
    "        mlflow.log_metrics({\"accuracy\": acc_churn, \"f1_score\": f1_churn})\n",
    "        mlflow.sklearn.log_model(rf_churn, \"model\", registered_model_name=\"셀러이탈예측\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.2 이상거래 탐지 (Isolation Forest)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[3.2] 이상거래 탐지 모델 (Isolation Forest)\")\n",
    "\n",
    "fraud_features = [\n",
    "    \"total_orders\", \"total_revenue\", \"product_count\", \"cs_tickets\",\n",
    "    \"refund_rate\", \"avg_response_time\",\n",
    "]\n",
    "X_fraud = seller_analytics_df[fraud_features].fillna(0).copy()\n",
    "scaler_fraud = StandardScaler()\n",
    "X_fraud_scaled = scaler_fraud.fit_transform(X_fraud)\n",
    "\n",
    "fraud_params = {\"n_estimators\": 150, \"contamination\": 0.05, \"random_state\": 42}\n",
    "iso_forest = IsolationForest(**fraud_params)\n",
    "fraud_pred = iso_forest.fit_predict(X_fraud_scaled)\n",
    "fraud_scores = iso_forest.decision_function(X_fraud_scaled)\n",
    "\n",
    "fraud_count = int((fraud_pred == -1).sum())\n",
    "print(f\"  이상 셀러: {fraud_count}명 ({fraud_count/len(fraud_pred)*100:.1f}%)\")\n",
    "\n",
    "seller_analytics_df[\"is_anomaly\"] = (fraud_pred == -1).astype(int)\n",
    "raw_sc = -fraud_scores\n",
    "norm_sc = (raw_sc - raw_sc.min()) / (raw_sc.max() - raw_sc.min() + 1e-8)\n",
    "seller_analytics_df[\"anomaly_score\"] = np.round(norm_sc, 4)\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    with mlflow.start_run(run_name=\"fraud_detection_model\"):\n",
    "        mlflow.set_tag(\"model_type\", \"anomaly_detection\")\n",
    "        mlflow.log_params(fraud_params)\n",
    "        mlflow.log_metrics({\n",
    "            \"anomaly_count\": fraud_count,\n",
    "            \"anomaly_ratio\": fraud_count / len(fraud_pred),\n",
    "        })\n",
    "        mlflow.sklearn.log_model(iso_forest, \"model\", registered_model_name=\"이상거래탐지\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.3 문의 자동 분류 (TF-IDF + RandomForest)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[3.3] 문의 자동 분류 모델 (TF-IDF + RandomForest)\")\n",
    "\n",
    "inquiry_texts = []\n",
    "inquiry_labels = []\n",
    "for cat, templates in CS_INQUIRY_TEMPLATES.items():\n",
    "    for tpl in templates:\n",
    "        for _ in range(20):\n",
    "            text = tpl.replace(\"{order_id}\", f\"ORD{rng.integers(10000, 99999)}\")\n",
    "            noise_words = rng.choice(\n",
    "                [\"요\", \"요.\", \"합니다.\", \"입니다.\", \"주세요.\", \"\"], size=1\n",
    "            )[0]\n",
    "            inquiry_texts.append(text + \" \" + noise_words)\n",
    "            inquiry_labels.append(cat)\n",
    "\n",
    "tfidf_inquiry = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_inquiry = tfidf_inquiry.fit_transform(inquiry_texts)\n",
    "le_inquiry_cat = LabelEncoder()\n",
    "y_inquiry = le_inquiry_cat.fit_transform(inquiry_labels)\n",
    "\n",
    "X_tr_inq, X_te_inq, y_tr_inq, y_te_inq = train_test_split(\n",
    "    X_inquiry, y_inquiry, test_size=0.2, random_state=42, stratify=y_inquiry,\n",
    ")\n",
    "\n",
    "inq_params = {\n",
    "    \"n_estimators\": 150, \"max_depth\": 10,\n",
    "    \"random_state\": 42, \"class_weight\": \"balanced\",\n",
    "}\n",
    "rf_inquiry = RandomForestClassifier(**inq_params, n_jobs=-1)\n",
    "rf_inquiry.fit(X_tr_inq, y_tr_inq)\n",
    "\n",
    "y_pred_inq = rf_inquiry.predict(X_te_inq)\n",
    "acc_inq = accuracy_score(y_te_inq, y_pred_inq)\n",
    "f1_inq = f1_score(y_te_inq, y_pred_inq, average=\"macro\")\n",
    "print(f\"  정확도: {acc_inq:.4f}, F1(매크로): {f1_inq:.4f}\")\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    with mlflow.start_run(run_name=\"inquiry_classification_model\"):\n",
    "        mlflow.set_tag(\"model_type\", \"text_classification\")\n",
    "        mlflow.log_params(inq_params)\n",
    "        mlflow.log_metrics({\"accuracy\": acc_inq, \"f1_macro\": f1_inq})\n",
    "        mlflow.sklearn.log_model(rf_inquiry, \"model\", registered_model_name=\"문의자동분류\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.4 셀러 세그먼트 (K-Means)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[3.4] 셀러 세그먼트 모델 (K-Means)\")\n",
    "\n",
    "segment_features = [\n",
    "    \"total_orders\", \"total_revenue\", \"product_count\",\n",
    "    \"cs_tickets\", \"refund_rate\", \"avg_response_time\",\n",
    "]\n",
    "X_seg = seller_analytics_df[segment_features].fillna(0).copy()\n",
    "scaler_cluster = StandardScaler()\n",
    "X_seg_scaled = scaler_cluster.fit_transform(X_seg)\n",
    "\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_seg_scaled)\n",
    "sil_score = silhouette_score(X_seg_scaled, cluster_labels)\n",
    "print(f\"  실루엣 점수: {sil_score:.4f}\")\n",
    "\n",
    "seller_analytics_df[\"cluster\"] = cluster_labels\n",
    "\n",
    "# 센트로이드 매출 기준으로 세그먼트 이름 자동 매핑\n",
    "centroid_revenue = []\n",
    "for c in range(n_clusters):\n",
    "    mask = seller_analytics_df[\"cluster\"] == c\n",
    "    avg_rev = seller_analytics_df.loc[mask, \"total_revenue\"].mean() if mask.any() else 0\n",
    "    avg_orders = seller_analytics_df.loc[mask, \"total_orders\"].mean() if mask.any() else 0\n",
    "    centroid_revenue.append((c, avg_rev, avg_orders))\n",
    "\n",
    "# 매출 내림차순 정렬\n",
    "centroid_revenue.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 매출 순서에 따라 이름 부여\n",
    "ORDERED_NAMES = [\"파워 셀러\", \"우수 셀러\", \"성장형 셀러\", \"관리 필요 셀러\", \"휴면 셀러\"]\n",
    "SEGMENT_NAMES = {}\n",
    "for rank, (cid, avg_rev, avg_ord) in enumerate(centroid_revenue):\n",
    "    SEGMENT_NAMES[cid] = ORDERED_NAMES[rank]\n",
    "    print(f\"  클러스터 {cid}: {ORDERED_NAMES[rank]} (평균 매출: {avg_rev:,.0f}, 평균 주문: {avg_ord:,.0f})\")\n",
    "\n",
    "# CSV에 세그먼트 이름 저장\n",
    "seller_analytics_df[\"segment_name\"] = seller_analytics_df[\"cluster\"].map(SEGMENT_NAMES)\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    with mlflow.start_run(run_name=\"seller_segmentation_model\"):\n",
    "        mlflow.set_tag(\"model_type\", \"clustering\")\n",
    "        mlflow.log_param(\"n_clusters\", n_clusters)\n",
    "        mlflow.log_metrics({\"silhouette_score\": sil_score})\n",
    "        mlflow.sklearn.log_model(kmeans, \"model\", registered_model_name=\"셀러세그먼트\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.5 매출 예측 (LightGBM / GradientBoosting)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[3.5] 매출 예측 모델 (LightGBM / GradientBoosting)\")\n",
    "\n",
    "revenue_data = []\n",
    "for _, seller in sellers_df.iterrows():\n",
    "    sa_row = seller_analytics_df[\n",
    "        seller_analytics_df[\"seller_id\"] == seller[\"seller_id\"]\n",
    "    ].iloc[0]\n",
    "    total_rev = seller[\"total_revenue\"]\n",
    "    txn_count = seller[\"total_orders\"]\n",
    "    unique_customers = max(1, int(txn_count * rng.uniform(0.4, 0.9)))\n",
    "    aov = total_rev / max(1, txn_count)\n",
    "    growth_rate = round(float(rng.normal(0.05, 0.15)), 4)\n",
    "\n",
    "    cat = shops_df[\n",
    "        shops_df[\"shop_id\"] == seller[\"seller_id\"].replace(\"SEL\", \"S\")\n",
    "    ][\"category\"].values\n",
    "    category_name = cat[0] if len(cat) > 0 else \"기타\"\n",
    "    industry_enc = CATEGORIES_KO.index(category_name) if category_name in CATEGORIES_KO else 0\n",
    "\n",
    "    reg = shops_df[\n",
    "        shops_df[\"shop_id\"] == seller[\"seller_id\"].replace(\"SEL\", \"S\")\n",
    "    ][\"region\"].values\n",
    "    region_name = reg[0] if len(reg) > 0 else \"서울\"\n",
    "    region_enc = REGIONS.index(region_name) if region_name in REGIONS else 0\n",
    "\n",
    "    target_rev = int(total_rev * (1 + growth_rate) + rng.normal(0, total_rev * 0.1))\n",
    "    target_rev = max(0, target_rev)\n",
    "\n",
    "    revenue_data.append({\n",
    "        \"total_revenue\": total_rev, \"txn_count\": txn_count,\n",
    "        \"unique_customers\": unique_customers, \"avg_order_value\": round(aov, 0),\n",
    "        \"revenue_growth_rate\": growth_rate, \"industry_encoded\": industry_enc,\n",
    "        \"region_encoded\": region_enc, \"target_revenue_next\": target_rev,\n",
    "    })\n",
    "\n",
    "revenue_df = pd.DataFrame(revenue_data)\n",
    "X_rev = revenue_df.drop(columns=[\"target_revenue_next\"]).copy()\n",
    "y_rev = revenue_df[\"target_revenue_next\"].copy()\n",
    "\n",
    "X_tr_rev, X_te_rev, y_tr_rev, y_te_rev = train_test_split(\n",
    "    X_rev, y_rev, test_size=0.2, random_state=42,\n",
    ")\n",
    "\n",
    "if LIGHTGBM_AVAILABLE:\n",
    "    model_revenue = lgb.LGBMRegressor(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.05,\n",
    "        num_leaves=31, random_state=42, verbose=-1,\n",
    "    )\n",
    "    algo_name_rev = \"LightGBM\"\n",
    "else:\n",
    "    model_revenue = GradientBoostingRegressor(\n",
    "        n_estimators=200, max_depth=6, learning_rate=0.05, random_state=42,\n",
    "    )\n",
    "    algo_name_rev = \"GradientBoosting\"\n",
    "\n",
    "model_revenue.fit(X_tr_rev, y_tr_rev)\n",
    "y_pred_rev = model_revenue.predict(X_te_rev)\n",
    "mae_rev = mean_absolute_error(y_te_rev, y_pred_rev)\n",
    "r2_rev = r2_score(y_te_rev, y_pred_rev)\n",
    "print(f\"  알고리즘: {algo_name_rev}\")\n",
    "print(f\"  MAE: {mae_rev:,.0f}, R2: {r2_rev:.4f}\")\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    with mlflow.start_run(run_name=\"revenue_prediction_model\"):\n",
    "        mlflow.set_tag(\"model_type\", \"regression\")\n",
    "        mlflow.set_tag(\"algorithm\", algo_name_rev)\n",
    "        mlflow.log_metrics({\"mae\": mae_rev, \"r2\": r2_rev})\n",
    "        mlflow.sklearn.log_model(model_revenue, \"model\", registered_model_name=\"매출예측\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.6 CS 응답 품질 (RandomForest Classifier)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[3.6] CS 응답 품질 모델 (RandomForest Classifier)\")\n",
    "\n",
    "cs_quality_data = []\n",
    "TICKET_CATS = [\"배송\", \"환불\", \"결제\", \"상품\", \"계정\", \"정산\", \"기술지원\"]\n",
    "SELLER_TIERS_CS = [\"Basic\", \"Standard\", \"Premium\", \"Enterprise\"]\n",
    "PRIORITIES = [\"urgent\", \"high\", \"normal\", \"low\"]\n",
    "PRIORITY_WEIGHTS = [0.10, 0.20, 0.45, 0.25]\n",
    "\n",
    "for _ in range(2000):\n",
    "    t_cat = rng.choice(TICKET_CATS)\n",
    "    s_tier = rng.choice(SELLER_TIERS_CS)\n",
    "    sentiment = round(float(rng.uniform(-1, 1)), 3)\n",
    "    order_val = int(rng.lognormal(10.5, 0.8))\n",
    "    is_repeat = int(rng.random() < 0.25)\n",
    "    text_len = int(rng.integers(20, 500))\n",
    "\n",
    "    base_idx = TICKET_CATS.index(t_cat)\n",
    "    tier_idx = SELLER_TIERS_CS.index(s_tier)\n",
    "\n",
    "    if t_cat in (\"환불\", \"결제\") and sentiment < -0.3:\n",
    "        priority = rng.choice([\"urgent\", \"high\"], p=[0.5, 0.5])\n",
    "    elif is_repeat:\n",
    "        priority = rng.choice([\"urgent\", \"high\", \"normal\"], p=[0.3, 0.4, 0.3])\n",
    "    else:\n",
    "        priority = rng.choice(PRIORITIES, p=PRIORITY_WEIGHTS)\n",
    "\n",
    "    cs_quality_data.append({\n",
    "        \"ticket_category_encoded\": base_idx,\n",
    "        \"seller_tier_encoded\": tier_idx,\n",
    "        \"sentiment_score\": sentiment,\n",
    "        \"order_value\": order_val,\n",
    "        \"is_repeat_issue\": is_repeat,\n",
    "        \"text_length\": text_len,\n",
    "        \"priority\": priority,\n",
    "    })\n",
    "\n",
    "cs_quality_df = pd.DataFrame(cs_quality_data)\n",
    "le_ticket_cat = LabelEncoder()\n",
    "le_seller_tier = LabelEncoder()\n",
    "le_cs_priority = LabelEncoder()\n",
    "\n",
    "le_ticket_cat.fit(TICKET_CATS)\n",
    "le_seller_tier.fit(SELLER_TIERS_CS)\n",
    "le_cs_priority.fit(PRIORITIES)\n",
    "\n",
    "cs_feat_cols = [\n",
    "    \"ticket_category_encoded\", \"seller_tier_encoded\", \"sentiment_score\",\n",
    "    \"order_value\", \"is_repeat_issue\", \"text_length\",\n",
    "]\n",
    "X_cs = cs_quality_df[cs_feat_cols].copy()\n",
    "y_cs = le_cs_priority.transform(cs_quality_df[\"priority\"])\n",
    "\n",
    "X_tr_cs, X_te_cs, y_tr_cs, y_te_cs = train_test_split(\n",
    "    X_cs, y_cs, test_size=0.2, random_state=42, stratify=y_cs,\n",
    ")\n",
    "\n",
    "cs_params = {\n",
    "    \"n_estimators\": 150, \"max_depth\": 10,\n",
    "    \"random_state\": 42, \"class_weight\": \"balanced\",\n",
    "}\n",
    "rf_cs = RandomForestClassifier(**cs_params, n_jobs=-1)\n",
    "rf_cs.fit(X_tr_cs, y_tr_cs)\n",
    "\n",
    "y_pred_cs = rf_cs.predict(X_te_cs)\n",
    "acc_cs = accuracy_score(y_te_cs, y_pred_cs)\n",
    "f1_cs = f1_score(y_te_cs, y_pred_cs, average=\"macro\")\n",
    "print(f\"  정확도: {acc_cs:.4f}, F1(매크로): {f1_cs:.4f}\")\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    with mlflow.start_run(run_name=\"cs_quality_model\"):\n",
    "        mlflow.set_tag(\"model_type\", \"classification\")\n",
    "        mlflow.log_params(cs_params)\n",
    "        mlflow.log_metrics({\"accuracy\": acc_cs, \"f1_macro\": f1_cs})\n",
    "        mlflow.sklearn.log_model(rf_cs, \"model\", registered_model_name=\"CS응답품질\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.7 고객 LTV 예측 (GradientBoosting Regressor)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[3.7] 고객 LTV 예측 모델 (GradientBoosting Regressor)\")\n",
    "\n",
    "n_customers = 3000\n",
    "ltv_data = []\n",
    "for i in range(n_customers):\n",
    "    purchase_count = int(np.clip(rng.poisson(5), 1, 50))\n",
    "    avg_ov = int(rng.lognormal(10.2, 0.6))\n",
    "    total_purchases = purchase_count * avg_ov\n",
    "    days_reg = int(rng.integers(30, 730))\n",
    "    return_rate = round(float(rng.beta(2, 15)), 4)\n",
    "    recency = int(rng.integers(0, min(days_reg, 180)))\n",
    "\n",
    "    ltv = total_purchases * (1 + 0.3 * np.log1p(purchase_count)) * (1 - return_rate * 0.5)\n",
    "    ltv *= rng.uniform(0.8, 1.2)\n",
    "    ltv = max(0, int(ltv))\n",
    "\n",
    "    ltv_data.append({\n",
    "        \"total_purchases\": total_purchases,\n",
    "        \"purchase_count\": purchase_count,\n",
    "        \"avg_order_value\": avg_ov,\n",
    "        \"days_since_register\": days_reg,\n",
    "        \"return_rate\": return_rate,\n",
    "        \"recency_days\": recency,\n",
    "        \"ltv\": ltv,\n",
    "    })\n",
    "\n",
    "ltv_df = pd.DataFrame(ltv_data)\n",
    "X_ltv = ltv_df.drop(columns=[\"ltv\"]).copy()\n",
    "y_ltv = ltv_df[\"ltv\"].copy()\n",
    "\n",
    "X_tr_ltv, X_te_ltv, y_tr_ltv, y_te_ltv = train_test_split(\n",
    "    X_ltv, y_ltv, test_size=0.2, random_state=42,\n",
    ")\n",
    "\n",
    "model_ltv = GradientBoostingRegressor(\n",
    "    n_estimators=200, max_depth=5, learning_rate=0.05, random_state=42,\n",
    ")\n",
    "model_ltv.fit(X_tr_ltv, y_tr_ltv)\n",
    "\n",
    "y_pred_ltv = model_ltv.predict(X_te_ltv)\n",
    "mae_ltv = mean_absolute_error(y_te_ltv, y_pred_ltv)\n",
    "r2_ltv = r2_score(y_te_ltv, y_pred_ltv)\n",
    "print(f\"  MAE: {mae_ltv:,.0f}, R2: {r2_ltv:.4f}\")\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    with mlflow.start_run(run_name=\"customer_ltv_model\"):\n",
    "        mlflow.set_tag(\"model_type\", \"regression\")\n",
    "        mlflow.log_metrics({\"mae\": mae_ltv, \"r2\": r2_ltv})\n",
    "        mlflow.sklearn.log_model(model_ltv, \"model\", registered_model_name=\"고객LTV예측\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.8 리뷰 감성 분석 (TF-IDF + LogisticRegression)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[3.8] 리뷰 감성 분석 모델 (TF-IDF + LogisticRegression)\")\n",
    "\n",
    "review_texts = []\n",
    "review_labels = []\n",
    "for tpl in REVIEW_TEMPLATES_POSITIVE:\n",
    "    for _ in range(30):\n",
    "        noise = rng.choice(\n",
    "            [\"\", \" 감사합니다!\", \" 좋아요!\", \" 강추!\", \" 최고에요!\"],\n",
    "            p=[0.3, 0.2, 0.2, 0.15, 0.15],\n",
    "        )\n",
    "        review_texts.append(tpl + noise)\n",
    "        review_labels.append(\"positive\")\n",
    "\n",
    "for tpl in REVIEW_TEMPLATES_NEGATIVE:\n",
    "    for _ in range(30):\n",
    "        noise = rng.choice(\n",
    "            [\"\", \" 실망이에요.\", \" 별로에요.\", \" 비추입니다.\", \"\"],\n",
    "            p=[0.3, 0.2, 0.2, 0.15, 0.15],\n",
    "        )\n",
    "        review_texts.append(tpl + noise)\n",
    "        review_labels.append(\"negative\")\n",
    "\n",
    "for tpl in REVIEW_TEMPLATES_NEUTRAL:\n",
    "    for _ in range(30):\n",
    "        noise = rng.choice(\n",
    "            [\"\", \" 그래요.\", \" 보통이에요.\", \"\"],\n",
    "            p=[0.3, 0.25, 0.25, 0.2],\n",
    "        )\n",
    "        review_texts.append(tpl + noise)\n",
    "        review_labels.append(\"neutral\")\n",
    "\n",
    "tfidf_sentiment = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_sent = tfidf_sentiment.fit_transform(review_texts)\n",
    "le_sentiment = LabelEncoder()\n",
    "y_sent = le_sentiment.fit_transform(review_labels)\n",
    "\n",
    "X_tr_sent, X_te_sent, y_tr_sent, y_te_sent = train_test_split(\n",
    "    X_sent, y_sent, test_size=0.2, random_state=42, stratify=y_sent,\n",
    ")\n",
    "\n",
    "model_sentiment = LogisticRegression(\n",
    "    max_iter=1000, random_state=42, class_weight=\"balanced\",\n",
    ")\n",
    "model_sentiment.fit(X_tr_sent, y_tr_sent)\n",
    "\n",
    "y_pred_sent = model_sentiment.predict(X_te_sent)\n",
    "acc_sent = accuracy_score(y_te_sent, y_pred_sent)\n",
    "f1_sent = f1_score(y_te_sent, y_pred_sent, average=\"macro\")\n",
    "print(f\"  정확도: {acc_sent:.4f}, F1(매크로): {f1_sent:.4f}\")\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    with mlflow.start_run(run_name=\"review_sentiment_model\"):\n",
    "        mlflow.set_tag(\"model_type\", \"text_classification\")\n",
    "        mlflow.log_metrics({\"accuracy\": acc_sent, \"f1_macro\": f1_sent})\n",
    "        mlflow.sklearn.log_model(\n",
    "            model_sentiment, \"model\", registered_model_name=\"리뷰감성분석\",\n",
    "        )\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.9 상품 수요 예측 (XGBoost / GradientBoosting)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[3.9] 상품 수요 예측 모델 (XGBoost / GradientBoosting)\")\n",
    "\n",
    "demand_data = []\n",
    "for _ in range(2000):\n",
    "    w1 = int(rng.poisson(15))\n",
    "    w2 = int(rng.poisson(max(1, w1 + rng.integers(-5, 5))))\n",
    "    w3 = int(rng.poisson(max(1, w2 + rng.integers(-5, 5))))\n",
    "    w4 = int(rng.poisson(max(1, w3 + rng.integers(-5, 5))))\n",
    "    price = int(rng.lognormal(10, 0.6))\n",
    "    cat_enc = int(rng.integers(0, len(CATEGORIES_KO)))\n",
    "    is_promo = int(rng.random() < 0.2)\n",
    "    review_count = int(rng.integers(0, 200))\n",
    "\n",
    "    trend = (w4 - w1) / max(1, w1)\n",
    "    base_demand = (w1 + w2 + w3 + w4) / 4\n",
    "    next_demand = int(max(0, base_demand * (1 + trend * 0.3 + is_promo * 0.2) + rng.normal(0, 3)))\n",
    "\n",
    "    demand_data.append({\n",
    "        \"week1_orders\": w1, \"week2_orders\": w2,\n",
    "        \"week3_orders\": w3, \"week4_orders\": w4,\n",
    "        \"price\": price, \"category_encoded\": cat_enc,\n",
    "        \"is_promotion\": is_promo, \"review_count\": review_count,\n",
    "        \"next_week_demand\": next_demand,\n",
    "    })\n",
    "\n",
    "demand_df = pd.DataFrame(demand_data)\n",
    "X_demand = demand_df.drop(columns=[\"next_week_demand\"]).copy()\n",
    "y_demand = demand_df[\"next_week_demand\"].copy()\n",
    "\n",
    "X_tr_dem, X_te_dem, y_tr_dem, y_te_dem = train_test_split(\n",
    "    X_demand, y_demand, test_size=0.2, random_state=42,\n",
    ")\n",
    "\n",
    "if XGBOOST_AVAILABLE:\n",
    "    model_demand = xgb.XGBRegressor(\n",
    "        n_estimators=200, max_depth=5, learning_rate=0.05,\n",
    "        random_state=42, verbosity=0,\n",
    "    )\n",
    "    algo_name_dem = \"XGBoost\"\n",
    "else:\n",
    "    model_demand = GradientBoostingRegressor(\n",
    "        n_estimators=200, max_depth=5, learning_rate=0.05, random_state=42,\n",
    "    )\n",
    "    algo_name_dem = \"GradientBoosting\"\n",
    "\n",
    "model_demand.fit(X_tr_dem, y_tr_dem)\n",
    "y_pred_dem = model_demand.predict(X_te_dem)\n",
    "mae_dem = mean_absolute_error(y_te_dem, y_pred_dem)\n",
    "r2_dem = r2_score(y_te_dem, y_pred_dem)\n",
    "print(f\"  알고리즘: {algo_name_dem}\")\n",
    "print(f\"  MAE: {mae_dem:.2f}, R2: {r2_dem:.4f}\")\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    with mlflow.start_run(run_name=\"demand_forecast_model\"):\n",
    "        mlflow.set_tag(\"model_type\", \"regression\")\n",
    "        mlflow.set_tag(\"algorithm\", algo_name_dem)\n",
    "        mlflow.log_metrics({\"mae\": mae_dem, \"r2\": r2_dem})\n",
    "        mlflow.sklearn.log_model(\n",
    "            model_demand, \"model\", registered_model_name=\"상품수요예측\",\n",
    "        )\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3.10 정산 이상 탐지 (DBSCAN)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[3.10] 정산 이상 탐지 모델 (DBSCAN)\")\n",
    "\n",
    "# 셀러당 3건 고정 (데모용 - 총 ~950건)\n",
    "total_n = len(sellers_df) * 3\n",
    "seller_ids_repeat = np.repeat(sellers_df[\"seller_id\"].values, 3)\n",
    "\n",
    "normal_df = pd.DataFrame({\n",
    "    \"seller_id\": seller_ids_repeat,\n",
    "    \"amount\": rng.lognormal(10.5, 0.7, size=total_n).astype(int),\n",
    "    \"fee_rate\": np.round(rng.uniform(0.02, 0.04, size=total_n), 4),\n",
    "    \"settlement_days\": np.round(np.clip(rng.exponential(3, size=total_n) + 1, 1, 14), 1),\n",
    "    \"txn_count\": rng.integers(1, 20, size=total_n),\n",
    "    \"refund_ratio\": np.round(rng.beta(1, 15, size=total_n), 4),\n",
    "})\n",
    "\n",
    "# 이상 데이터 50건 고정\n",
    "anomaly_df = pd.DataFrame({\n",
    "    \"seller_id\": rng.choice(sellers_df[\"seller_id\"].values, size=50),\n",
    "    \"amount\": rng.integers(500000, 5000000, size=50),\n",
    "    \"fee_rate\": np.round(rng.uniform(0.08, 0.15, size=50), 4),\n",
    "    \"settlement_days\": np.round(rng.uniform(10, 30, size=50), 1),\n",
    "    \"txn_count\": rng.integers(50, 200, size=50),\n",
    "    \"refund_ratio\": np.round(rng.uniform(0.3, 0.8, size=50), 4),\n",
    "})\n",
    "settlement_df = pd.concat([normal_df, anomaly_df], ignore_index=True)\n",
    "settle_features = [\"amount\", \"fee_rate\", \"settlement_days\", \"txn_count\", \"refund_ratio\"]\n",
    "X_settle = settlement_df[settle_features].copy()\n",
    "scaler_settle = StandardScaler()\n",
    "X_settle_scaled = scaler_settle.fit_transform(X_settle)\n",
    "\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=5)\n",
    "settle_labels = dbscan.fit_predict(X_settle_scaled)\n",
    "\n",
    "n_noise = int((settle_labels == -1).sum())\n",
    "n_clusters_db = len(set(settle_labels)) - (1 if -1 in settle_labels else 0)\n",
    "print(f\"  클러스터 수: {n_clusters_db}, 이상(noise): {n_noise}건 ({n_noise/len(settle_labels)*100:.1f}%)\")\n",
    "\n",
    "settlement_df[\"cluster_label\"] = settle_labels\n",
    "settlement_df[\"is_anomaly\"] = (settle_labels == -1).astype(int)\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    with mlflow.start_run(run_name=\"settlement_anomaly_model\"):\n",
    "        mlflow.set_tag(\"model_type\", \"anomaly_detection\")\n",
    "        mlflow.log_param(\"eps\", 1.5)\n",
    "        mlflow.log_param(\"min_samples\", 5)\n",
    "        mlflow.log_metrics({\"n_clusters\": n_clusters_db, \"n_noise\": n_noise})\n",
    "        mlflow.sklearn.log_model(dbscan, \"model\", registered_model_name=\"정산이상탐지\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 3 완료: 모든 모델 학습 완료 (10개)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: 저장 및 테스트\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PART 4: 저장 및 테스트\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4.1 CSV 저장 (17개)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[4.1] CSV 파일 저장 (17개)\")\n",
    "\n",
    "csv_enc = \"utf-8-sig\"\n",
    "shops_df.to_csv(BACKEND_DIR / \"shops.csv\", index=False, encoding=csv_enc)\n",
    "categories_df.to_csv(BACKEND_DIR / \"categories.csv\", index=False, encoding=csv_enc)\n",
    "services_df.to_csv(BACKEND_DIR / \"services.csv\", index=False, encoding=csv_enc)\n",
    "products_df.to_csv(BACKEND_DIR / \"products.csv\", index=False, encoding=csv_enc)\n",
    "sellers_df.to_csv(BACKEND_DIR / \"sellers.csv\", index=False, encoding=csv_enc)\n",
    "op_logs_df.to_csv(BACKEND_DIR / \"operation_logs.csv\", index=False, encoding=csv_enc)\n",
    "seller_analytics_df.to_csv(BACKEND_DIR / \"seller_analytics.csv\", index=False, encoding=csv_enc)\n",
    "shop_perf_df.to_csv(BACKEND_DIR / \"shop_performance.csv\", index=False, encoding=csv_enc)\n",
    "daily_metrics_df.to_csv(BACKEND_DIR / \"daily_metrics.csv\", index=False, encoding=csv_enc)\n",
    "cs_stats_df.to_csv(BACKEND_DIR / \"cs_stats.csv\", index=False, encoding=csv_enc)\n",
    "fraud_df.to_csv(BACKEND_DIR / \"fraud_details.csv\", index=False, encoding=csv_enc)\n",
    "cohort_df.to_csv(BACKEND_DIR / \"cohort_retention.csv\", index=False, encoding=csv_enc)\n",
    "conversion_df.to_csv(BACKEND_DIR / \"conversion_funnel.csv\", index=False, encoding=csv_enc)\n",
    "seller_activity_df.to_csv(BACKEND_DIR / \"seller_activity.csv\", index=False, encoding=csv_enc)\n",
    "platform_docs_df.to_csv(BACKEND_DIR / \"platform_docs.csv\", index=False, encoding=csv_enc)\n",
    "glossary_df.to_csv(BACKEND_DIR / \"ecommerce_glossary.csv\", index=False, encoding=csv_enc)\n",
    "seller_products_df.to_csv(BACKEND_DIR / \"seller_products.csv\", index=False, encoding=csv_enc)\n",
    "seller_resources_df.to_csv(BACKEND_DIR / \"seller_resources.csv\", index=False, encoding=csv_enc)\n",
    "print(\"  18개 CSV 파일 저장 완료\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4.2 모델 저장 (10개 + 보조 파일)\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[4.2] 모델 파일 저장\")\n",
    "\n",
    "# 1. 셀러 이탈 예측\n",
    "joblib.dump(rf_churn, BACKEND_DIR / \"model_seller_churn.pkl\")\n",
    "if SHAP_AVAILABLE and shap_explainer is not None:\n",
    "    joblib.dump(shap_explainer, BACKEND_DIR / \"shap_explainer_churn.pkl\")\n",
    "    print(\"  - shap_explainer_churn.pkl 저장 완료\")\n",
    "\n",
    "churn_config = {\n",
    "    \"features\": CHURN_FEATURES,\n",
    "    \"feature_names_kr\": CHURN_FEATURE_NAMES_KR,\n",
    "    \"feature_importances\": {k: float(v) for k, v in feature_importances_churn.items()},\n",
    "    \"shap_available\": SHAP_AVAILABLE,\n",
    "    \"model_accuracy\": float(acc_churn),\n",
    "    \"model_f1\": float(f1_churn),\n",
    "}\n",
    "with open(BACKEND_DIR / \"churn_model_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(churn_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 2. 이상거래 탐지\n",
    "joblib.dump(iso_forest, BACKEND_DIR / \"model_fraud_detection.pkl\")\n",
    "\n",
    "# 3. 문의 자동 분류\n",
    "joblib.dump(rf_inquiry, BACKEND_DIR / \"model_inquiry_classification.pkl\")\n",
    "joblib.dump(tfidf_inquiry, BACKEND_DIR / \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(le_inquiry_cat, BACKEND_DIR / \"le_inquiry_category.pkl\")\n",
    "\n",
    "# 4. 셀러 세그먼트\n",
    "joblib.dump(kmeans, BACKEND_DIR / \"model_seller_segment.pkl\")\n",
    "joblib.dump(scaler_cluster, BACKEND_DIR / \"scaler_cluster.pkl\")\n",
    "\n",
    "# 5. 매출 예측\n",
    "joblib.dump(model_revenue, BACKEND_DIR / \"model_revenue_prediction.pkl\")\n",
    "revenue_config = {\n",
    "    \"algorithm\": algo_name_rev,\n",
    "    \"features\": list(X_rev.columns),\n",
    "    \"mae\": float(mae_rev),\n",
    "    \"r2_score\": float(r2_rev),\n",
    "}\n",
    "with open(BACKEND_DIR / \"revenue_model_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(revenue_config, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 6. CS 응답 품질\n",
    "joblib.dump(rf_cs, BACKEND_DIR / \"model_cs_quality.pkl\")\n",
    "joblib.dump(le_ticket_cat, BACKEND_DIR / \"le_ticket_category.pkl\")\n",
    "joblib.dump(le_seller_tier, BACKEND_DIR / \"le_seller_tier.pkl\")\n",
    "joblib.dump(le_cs_priority, BACKEND_DIR / \"le_cs_priority.pkl\")\n",
    "\n",
    "# 7. 고객 LTV\n",
    "joblib.dump(model_ltv, BACKEND_DIR / \"model_customer_ltv.pkl\")\n",
    "\n",
    "# 8. 리뷰 감성 분석\n",
    "joblib.dump(model_sentiment, BACKEND_DIR / \"model_review_sentiment.pkl\")\n",
    "joblib.dump(tfidf_sentiment, BACKEND_DIR / \"tfidf_vectorizer_sentiment.pkl\")\n",
    "\n",
    "# 9. 상품 수요 예측\n",
    "joblib.dump(model_demand, BACKEND_DIR / \"model_demand_forecast.pkl\")\n",
    "\n",
    "# 10. 정산 이상 탐지\n",
    "joblib.dump(dbscan, BACKEND_DIR / \"model_settlement_anomaly.pkl\")\n",
    "\n",
    "print(\"  10개 모델 + 보조 파일 저장 완료\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4.2.1 Guardian 감사 로그 이상탐지 (Isolation Forest) 학습\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[4.2.1] Guardian 감사 로그 IsolationForest 학습\")\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "GUARDIAN_DB_PATH = BACKEND_DIR / \"guardian.db\"\n",
    "CORE_TABLES = {\"orders\", \"payments\", \"users\", \"products\", \"shipments\"}\n",
    "\n",
    "# (A) 감사 로그 DB 준비 (없으면 생성 + 시드 데이터)\n",
    "def _prepare_guardian_db(db_path):\n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"\"\"CREATE TABLE IF NOT EXISTS audit_log (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        timestamp TEXT NOT NULL,\n",
    "        user_id TEXT NOT NULL,\n",
    "        action TEXT NOT NULL,\n",
    "        table_name TEXT NOT NULL,\n",
    "        row_count INTEGER DEFAULT 0,\n",
    "        affected_amount REAL DEFAULT 0,\n",
    "        status TEXT DEFAULT 'executed',\n",
    "        risk_level TEXT DEFAULT 'LOW',\n",
    "        agent_reason TEXT DEFAULT '',\n",
    "        created_at TEXT DEFAULT CURRENT_TIMESTAMP\n",
    "    )\"\"\")\n",
    "    c.execute(\"\"\"CREATE TABLE IF NOT EXISTS incidents (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        action TEXT NOT NULL,\n",
    "        table_name TEXT NOT NULL,\n",
    "        row_count INTEGER,\n",
    "        was_mistake INTEGER DEFAULT 0,\n",
    "        description TEXT\n",
    "    )\"\"\")\n",
    "\n",
    "    existing = c.execute(\"SELECT COUNT(*) FROM audit_log\").fetchone()[0]\n",
    "    if existing < 200:\n",
    "        from datetime import datetime, timedelta\n",
    "        _now = datetime.now()\n",
    "        _users = [\"kim\", \"park\", \"lee\", \"choi\", \"jung\"]\n",
    "        _tables = [\"orders\", \"payments\", \"users\", \"products\", \"shipments\", \"logs\", \"temp_reports\"]\n",
    "        for _ in range(300):\n",
    "            ts = (_now - timedelta(days=int(rng.integers(0, 60)), hours=int(rng.integers(0, 24)))).isoformat()\n",
    "            u = rng.choice(_users)\n",
    "            act = rng.choice([\"INSERT\", \"UPDATE\", \"DELETE\", \"SELECT\"])\n",
    "            tbl = rng.choice(_tables)\n",
    "            rc = int(rng.integers(1, 8)) if act == \"DELETE\" else int(rng.integers(1, 30))\n",
    "            amt = rc * int(rng.integers(30000, 120000)) if tbl in (\"orders\", \"payments\") else 0\n",
    "            c.execute(\n",
    "                \"INSERT INTO audit_log (timestamp,user_id,action,table_name,row_count,affected_amount,status,risk_level) \"\n",
    "                \"VALUES (?,?,?,?,?,?,?,?)\",\n",
    "                (ts, u, act, tbl, rc, amt, \"executed\", \"LOW\")\n",
    "            )\n",
    "        # 이상 패턴 데이터 (야간 대량 DELETE 등)\n",
    "        for _ in range(20):\n",
    "            ts = (_now - timedelta(days=int(rng.integers(0, 30)))).replace(hour=int(rng.integers(22, 24))).isoformat()\n",
    "            u = rng.choice([\"unknown_admin\", \"temp_user\", rng.choice(_users)])\n",
    "            tbl = rng.choice([\"orders\", \"payments\", \"users\"])\n",
    "            rc = int(rng.integers(100, 5000))\n",
    "            amt = rc * int(rng.integers(50000, 150000))\n",
    "            c.execute(\n",
    "                \"INSERT INTO audit_log (timestamp,user_id,action,table_name,row_count,affected_amount,status,risk_level) \"\n",
    "                \"VALUES (?,?,?,?,?,?,?,?)\",\n",
    "                (ts, u, \"DELETE\", tbl, rc, amt, \"executed\", \"HIGH\")\n",
    "            )\n",
    "        # 과거 사건 데이터\n",
    "        _incidents = [\n",
    "            (\"DELETE\", \"orders\", 250, 1, \"신입 직원이 WHERE 없이 DELETE 실행, 전체 복구\"),\n",
    "            (\"DELETE\", \"orders\", 180, 1, \"테스트 DB와 혼동하여 프로덕션에서 삭제\"),\n",
    "            (\"DELETE\", \"payments\", 320, 1, \"정산 데이터 삭제 실수, DBA가 백업에서 복구\"),\n",
    "            (\"DELETE\", \"orders\", 150, 1, \"퇴근 전 급하게 작업하다 실수\"),\n",
    "            (\"DELETE\", \"users\", 500, 1, \"탈퇴 처리 스크립트 오류로 활성 유저 삭제\"),\n",
    "            (\"DELETE\", \"products\", 200, 1, \"카테고리 정리 중 실수로 전체 삭제\"),\n",
    "            (\"DELETE\", \"orders\", 400, 1, \"연말 정산 중 데이터 혼동\"),\n",
    "            (\"DELETE\", \"logs\", 10000, 0, \"정기 로그 정리 (스케줄 작업)\"),\n",
    "            (\"DELETE\", \"temp_reports\", 5000, 0, \"임시 리포트 정리\"),\n",
    "            (\"UPDATE\", \"orders\", 300, 1, \"금액 필드 일괄 0으로 업데이트 실수\"),\n",
    "            (\"UPDATE\", \"products\", 150, 1, \"가격 일괄 변경 시 WHERE 조건 누락\"),\n",
    "            (\"UPDATE\", \"users\", 1000, 1, \"권한 일괄 변경 실수\"),\n",
    "        ]\n",
    "        for act, tbl, rc, mis, desc in _incidents:\n",
    "            c.execute(\"INSERT INTO incidents (action,table_name,row_count,was_mistake,description) VALUES (?,?,?,?,?)\",\n",
    "                      (act, tbl, rc, mis, desc))\n",
    "        conn.commit()\n",
    "        print(f\"  guardian.db 시드 데이터 생성 완료 (audit_log: {c.execute('SELECT COUNT(*) FROM audit_log').fetchone()[0]}건)\")\n",
    "    else:\n",
    "        print(f\"  guardian.db 기존 데이터 사용 ({existing}건)\")\n",
    "    return conn\n",
    "\n",
    "guardian_conn = _prepare_guardian_db(GUARDIAN_DB_PATH)\n",
    "\n",
    "# (B) 학습 데이터 로드\n",
    "rows = guardian_conn.execute(\n",
    "    \"SELECT user_id, action, table_name, row_count, affected_amount, timestamp \"\n",
    "    \"FROM audit_log WHERE status='executed'\"\n",
    ").fetchall()\n",
    "guardian_conn.close()\n",
    "\n",
    "ACTION_MAP = {\"INSERT\": 0, \"SELECT\": 0, \"UPDATE\": 1, \"DELETE\": 2,\n",
    "              \"ALTER\": 3, \"DROP\": 4, \"TRUNCATE\": 4}\n",
    "guardian_features = []\n",
    "for r in rows:\n",
    "    ts = r[\"timestamp\"] or \"\"\n",
    "    hour = int(ts[11:13]) if len(ts) > 13 else 12\n",
    "    guardian_features.append([\n",
    "        ACTION_MAP.get(r[\"action\"], 0),\n",
    "        1 if r[\"table_name\"] in CORE_TABLES else 0,\n",
    "        r[\"row_count\"],\n",
    "        np.log1p(r[\"row_count\"]),\n",
    "        r[\"affected_amount\"],\n",
    "        hour,\n",
    "        1 if (hour >= 22 or hour < 6) else 0,\n",
    "    ])\n",
    "\n",
    "X_guardian = np.array(guardian_features)\n",
    "scaler_guardian = StandardScaler()\n",
    "X_guardian_scaled = scaler_guardian.fit_transform(X_guardian)\n",
    "\n",
    "# (C) IsolationForest 학습\n",
    "guardian_iso = IsolationForest(n_estimators=100, contamination=0.05, random_state=42)\n",
    "guardian_iso.fit(X_guardian_scaled)\n",
    "\n",
    "# 학습 결과 확인\n",
    "guardian_pred = guardian_iso.predict(X_guardian_scaled)\n",
    "guardian_anomaly_count = int((guardian_pred == -1).sum())\n",
    "print(f\"  학습 데이터: {len(rows)}건, 7 features\")\n",
    "print(f\"  이상 탐지: {guardian_anomaly_count}건 ({guardian_anomaly_count/len(rows)*100:.1f}%)\")\n",
    "\n",
    "# (D) 모델 저장\n",
    "joblib.dump(guardian_iso, BACKEND_DIR / \"model_guardian_anomaly.pkl\")\n",
    "joblib.dump(scaler_guardian, BACKEND_DIR / \"scaler_guardian.pkl\")\n",
    "print(\"  model_guardian_anomaly.pkl, scaler_guardian.pkl 저장 완료\")\n",
    "\n",
    "if MLFLOW_AVAILABLE:\n",
    "    with mlflow.start_run(run_name=\"guardian_anomaly_model\"):\n",
    "        mlflow.set_tag(\"model_type\", \"anomaly_detection\")\n",
    "        mlflow.log_params({\"n_estimators\": 100, \"contamination\": 0.05, \"random_state\": 42})\n",
    "        mlflow.log_metrics({\n",
    "            \"anomaly_count\": guardian_anomaly_count,\n",
    "            \"anomaly_ratio\": guardian_anomaly_count / len(rows),\n",
    "            \"n_samples\": len(rows),\n",
    "        })\n",
    "        mlflow.sklearn.log_model(guardian_iso, \"model\", registered_model_name=\"Guardian감사로그이상탐지\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4.3 셀러별 예측 결과 사전계산 → seller_analytics.csv 에 추가\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[4.3] 셀러별 모델 예측 결과 사전계산\")\n",
    "\n",
    "# (A) 매출 예측: 전 셀러에 대해 model_revenue로 예측\n",
    "rev_feat_cols = [\"total_revenue\", \"txn_count\", \"unique_customers\",\n",
    "                 \"avg_order_value\", \"revenue_growth_rate\",\n",
    "                 \"industry_encoded\", \"region_encoded\"]\n",
    "predicted_revenues = []\n",
    "revenue_growth_rates = []\n",
    "for idx, seller in sellers_df.iterrows():\n",
    "    sa_row = seller_analytics_df[\n",
    "        seller_analytics_df[\"seller_id\"] == seller[\"seller_id\"]\n",
    "    ]\n",
    "    if len(sa_row) == 0:\n",
    "        predicted_revenues.append(0)\n",
    "        revenue_growth_rates.append(0.0)\n",
    "        continue\n",
    "    total_rev = seller[\"total_revenue\"]\n",
    "    txn_count = seller[\"total_orders\"]\n",
    "    unique_customers = max(1, int(txn_count * rng.uniform(0.4, 0.9)))\n",
    "    aov = total_rev / max(1, txn_count)\n",
    "    growth_rate = round(float(rng.normal(0.05, 0.15)), 4)\n",
    "    cat = shops_df[\n",
    "        shops_df[\"shop_id\"] == seller[\"seller_id\"].replace(\"SEL\", \"S\")\n",
    "    ][\"category\"].values\n",
    "    category_name = cat[0] if len(cat) > 0 else \"기타\"\n",
    "    industry_enc = CATEGORIES_KO.index(category_name) if category_name in CATEGORIES_KO else 0\n",
    "    reg = shops_df[\n",
    "        shops_df[\"shop_id\"] == seller[\"seller_id\"].replace(\"SEL\", \"S\")\n",
    "    ][\"region\"].values\n",
    "    region_name = reg[0] if len(reg) > 0 else \"서울\"\n",
    "    region_enc = REGIONS.index(region_name) if region_name in REGIONS else 0\n",
    "\n",
    "    X_pred = pd.DataFrame([{\n",
    "        \"total_revenue\": total_rev, \"txn_count\": txn_count,\n",
    "        \"unique_customers\": unique_customers, \"avg_order_value\": round(aov, 0),\n",
    "        \"revenue_growth_rate\": growth_rate, \"industry_encoded\": industry_enc,\n",
    "        \"region_encoded\": region_enc,\n",
    "    }])\n",
    "    pred_rev = int(model_revenue.predict(X_pred)[0])\n",
    "    pred_rev = max(0, pred_rev)\n",
    "    predicted_revenues.append(pred_rev)\n",
    "    actual_growth = round((pred_rev / max(1, total_rev) - 1) * 100, 1)\n",
    "    revenue_growth_rates.append(actual_growth)\n",
    "\n",
    "seller_analytics_df[\"predicted_revenue\"] = predicted_revenues\n",
    "seller_analytics_df[\"revenue_growth_rate\"] = revenue_growth_rates\n",
    "print(f\"  매출 예측 완료: {len(predicted_revenues)}명\")\n",
    "\n",
    "# (B) CS 응답 품질: 셀러별로 대표 티켓 생성 후 모델 예측 → 우선순위 분포 기반 점수화\n",
    "cs_scores = []\n",
    "cs_grades = []\n",
    "for _, seller in sellers_df.iterrows():\n",
    "    tier = seller.get(\"plan_tier\", \"Standard\")\n",
    "    tier_idx = SELLER_TIERS_CS.index(tier) if tier in SELLER_TIERS_CS else 1\n",
    "    refund_rate = float(seller.get(\"refund_rate\", 0.05))\n",
    "    avg_resp = float(seller.get(\"avg_response_time\", 2.0))\n",
    "    # 셀러별 대표 티켓 5개 생성하여 평균 우선순위 예측\n",
    "    sample_tickets = []\n",
    "    for t_idx in range(min(5, len(TICKET_CATS))):\n",
    "        sentiment = -0.5 + refund_rate * 3  # 환불률 높으면 부정적\n",
    "        order_val = int(seller.get(\"total_revenue\", 0) / max(1, seller.get(\"total_orders\", 1)))\n",
    "        is_repeat = 1 if refund_rate > 0.1 else 0\n",
    "        text_len = 150\n",
    "        sample_tickets.append({\n",
    "            \"ticket_category_encoded\": t_idx,\n",
    "            \"seller_tier_encoded\": tier_idx,\n",
    "            \"sentiment_score\": round(sentiment, 3),\n",
    "            \"order_value\": order_val,\n",
    "            \"is_repeat_issue\": is_repeat,\n",
    "            \"text_length\": text_len,\n",
    "        })\n",
    "    X_cs_pred = pd.DataFrame(sample_tickets)\n",
    "    preds = rf_cs.predict(X_cs_pred)\n",
    "    probas = rf_cs.predict_proba(X_cs_pred)\n",
    "    # urgent=0, high=1, normal=2, low=3 순 (le_cs_priority 기준)\n",
    "    # 점수 = low 비율 * 100 + normal 비율 * 70 + high 비율 * 40 + urgent 비율 * 10\n",
    "    avg_proba = probas.mean(axis=0)\n",
    "    priority_classes = le_cs_priority.classes_\n",
    "    score_map = {\"low\": 100, \"normal\": 70, \"high\": 40, \"urgent\": 10}\n",
    "    cs_score = 0\n",
    "    for i, cls in enumerate(priority_classes):\n",
    "        cs_score += avg_proba[i] * score_map.get(cls, 50)\n",
    "    cs_score = max(0, min(100, int(cs_score)))\n",
    "    cs_scores.append(cs_score)\n",
    "    cs_grades.append(\"우수\" if cs_score >= 80 else \"보통\" if cs_score >= 50 else \"개선필요\")\n",
    "\n",
    "seller_analytics_df[\"cs_quality_score\"] = cs_scores\n",
    "seller_analytics_df[\"cs_quality_grade\"] = cs_grades\n",
    "print(f\"  CS 품질 예측 완료: {len(cs_scores)}명\")\n",
    "\n",
    "# (C) 고객 LTV 예측: 셀러별 LTV\n",
    "ltv_predictions = []\n",
    "for _, seller in sellers_df.iterrows():\n",
    "    total_rev = seller[\"total_revenue\"]\n",
    "    total_ord = seller[\"total_orders\"]\n",
    "    aov = total_rev / max(1, total_ord)\n",
    "    months_active = max(1, seller.get(\"days_since_register\", 180) / 30)\n",
    "    monthly_rev = total_rev / months_active\n",
    "    X_ltv = pd.DataFrame([{\n",
    "        \"total_revenue\": total_rev, \"total_orders\": total_ord,\n",
    "        \"avg_order_value\": round(aov, 0), \"months_active\": round(months_active, 1),\n",
    "        \"monthly_revenue\": round(monthly_rev, 0),\n",
    "    }])\n",
    "    try:\n",
    "        ltv_pred = int(model_ltv.predict(X_ltv)[0])\n",
    "    except Exception:\n",
    "        ltv_pred = int(total_rev * 1.5)\n",
    "    ltv_predictions.append(max(0, ltv_pred))\n",
    "\n",
    "seller_analytics_df[\"predicted_ltv\"] = ltv_predictions\n",
    "print(f\"  LTV 예측 완료: {len(ltv_predictions)}명\")\n",
    "\n",
    "# CSV 다시 저장 (예측 결과 포함)\n",
    "seller_analytics_df.to_csv(BACKEND_DIR / \"seller_analytics.csv\", index=False, encoding=csv_enc)\n",
    "print(\"  seller_analytics.csv 업데이트 완료 (예측 컬럼 추가)\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4.4 예측 함수 테스트\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n[4.3] 예측 함수 테스트\")\n",
    "\n",
    "# 이탈 예측 테스트\n",
    "sample_churn = {f: 0 for f in CHURN_FEATURES}\n",
    "sample_churn.update({\n",
    "    \"total_orders\": 50, \"total_revenue\": 5000000, \"product_count\": 15,\n",
    "    \"cs_tickets\": 3, \"refund_rate\": 0.05, \"avg_response_time\": 2.5,\n",
    "    \"days_since_last_login\": 2, \"days_since_register\": 180, \"plan_tier_encoded\": 1,\n",
    "})\n",
    "X_sample_churn = pd.DataFrame([sample_churn])[CHURN_FEATURES]\n",
    "churn_pred = rf_churn.predict(X_sample_churn)[0]\n",
    "churn_prob = rf_churn.predict_proba(X_sample_churn)[0]\n",
    "print(f\"  셀러 이탈 예측: {'이탈' if churn_pred else '유지'} (확률: {churn_prob[1]:.2%})\")\n",
    "\n",
    "# 문의 분류 테스트\n",
    "test_inquiry = \"배송이 너무 늦어요 언제 오나요?\"\n",
    "X_inq_test = tfidf_inquiry.transform([test_inquiry])\n",
    "inq_pred = le_inquiry_cat.inverse_transform(rf_inquiry.predict(X_inq_test))[0]\n",
    "print(f\"  문의 분류: '{test_inquiry}' -> {inq_pred}\")\n",
    "\n",
    "# 감성 분석 테스트\n",
    "test_review = \"품질이 정말 좋고 배송도 빨라요! 추천합니다!\"\n",
    "X_rev_test = tfidf_sentiment.transform([test_review])\n",
    "sent_pred = le_sentiment.inverse_transform(model_sentiment.predict(X_rev_test))[0]\n",
    "print(f\"  감성 분석: '{test_review}' -> {sent_pred}\")\n",
    "\n",
    "# 세그먼트 테스트\n",
    "sample_seg = {\n",
    "    \"total_orders\": 200, \"total_revenue\": 30000000, \"product_count\": 30,\n",
    "    \"cs_tickets\": 5, \"refund_rate\": 0.03, \"avg_response_time\": 1.5,\n",
    "}\n",
    "X_seg_test = pd.DataFrame([sample_seg])[segment_features]\n",
    "X_seg_test_scaled = scaler_cluster.transform(X_seg_test)\n",
    "seg_pred = int(kmeans.predict(X_seg_test_scaled)[0])\n",
    "print(f\"  셀러 세그먼트: {SEGMENT_NAMES.get(seg_pred, '알 수 없음')}\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 완료 요약\n",
    "# --------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"완료! 카페24 이커머스 데이터 생성 및 모델 학습 성공\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n[요약]\")\n",
    "print(f\"  데이터:\")\n",
    "print(f\"    - 쇼핑몰: {len(shops_df)}개, 상품: {len(products_df)}개, 셀러: {len(sellers_df)}명\")\n",
    "print(f\"    - 운영 로그: {len(op_logs_df)}건, 서비스: {len(services_df)}건\")\n",
    "print(f\"    - 일별 지표: {len(daily_metrics_df)}일, 셀러 활동: {len(seller_activity_df)}건\")\n",
    "print(f\"    - CSV 파일: 17개\")\n",
    "print(f\"  모델 (11개):\")\n",
    "print(f\"    1. 셀러 이탈 예측 (RandomForest + SHAP)\")\n",
    "print(f\"    2. 이상거래 탐지 (Isolation Forest)\")\n",
    "print(f\"    3. 문의 자동 분류 (TF-IDF + RandomForest)\")\n",
    "print(f\"    4. 셀러 세그먼트 (K-Means)\")\n",
    "print(f\"    5. 매출 예측 ({algo_name_rev})\")\n",
    "print(f\"    6. CS 응답 품질 (RandomForest)\")\n",
    "print(f\"    7. 고객 LTV 예측 (GradientBoosting)\")\n",
    "print(f\"    8. 리뷰 감성 분석 (TF-IDF + LogisticRegression)\")\n",
    "print(f\"    9. 상품 수요 예측 ({algo_name_dem})\")\n",
    "print(f\"   10. 정산 이상 탐지 (DBSCAN)\")\n",
    "print(f\"   11. Guardian 감사 로그 이상탐지 (IsolationForest, {len(rows)}건)\")\n",
    "print(f\"  SHAP: {'활성화' if SHAP_AVAILABLE else '비활성화'}\")\n",
    "print(f\"  MLflow: {'활성화' if MLFLOW_AVAILABLE else '비활성화'}\")\n",
    "print(f\"\\n백엔드 서버 시작: cd \\\"backend 리팩토링 시작\\\" && python main.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08331ce8-beed-4b92-bc69-2a23bced259b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10",
   "language": "python",
   "name": "3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
